{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /aincome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/aincome\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/stable_baselines/__init__.py:33: UserWarning: stable-baselines is in maintenance mode, please use [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) for an up-to-date version. You can find a [migration guide](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html) in SB3 documentation.\n",
      "  \"stable-baselines is in maintenance mode, please use [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) for an up-to-date version. You can find a [migration guide](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html) in SB3 documentation.\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "#from tqdm.notebook import tqdm\n",
    "#from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import warnings\n",
    "import typing\n",
    "from typing import Union, List, Dict, Any, Optional\n",
    "\n",
    "import gym\n",
    "import gym_anytrading\n",
    "from gym_anytrading.envs import TradingEnv, ForexEnv, StocksEnv, Actions, Positions\n",
    "from gym_anytrading.datasets import FOREX_EURUSD_1H_ASK, STOCKS_GOOGL\n",
    "\n",
    "from stable_baselines.bench import Monitor\n",
    "\n",
    "from stable_baselines.common.vec_env import VecEnv, sync_envs_normalization, DummyVecEnv\n",
    "from stable_baselines.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common import make_vec_env\n",
    "from stable_baselines import PPO2\n",
    "from stable_baselines import ACKTR\n",
    "from stable_baselines import A2C\n",
    "from stable_baselines.common.callbacks import CallbackList, CheckpointCallback, EvalCallback, EventCallback\n",
    "from stable_baselines.common.callbacks import BaseCallback\n",
    "from stable_baselines.common.evaluation import evaluate_policy\n",
    "\n",
    "import quantstats as qs\n",
    "import mplfinance as mpf\n",
    "\n",
    "import talib\n",
    "from yahoo_finance_api2 import share\n",
    "import yfinance\n",
    "from Historic_Crypto import Cryptocurrencies\n",
    "from Historic_Crypto import HistoricalData\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引数情報\n",
    "symbol      = 'XLM-USD'          # 通貨レート\n",
    "granularity = 300              # 何秒置きにデータを取得するか(60, 300, 900, 3600, 21600, 86400) が指定可能\n",
    "start_date  = '2020-01-01-00-00' # データ取得範囲：開始日\n",
    "end_date    = '2020-01-29-00-00' # データ取得範囲：終了日\n",
    "\n",
    "dataset_name = \"./data/datasets/v1.0/symbol-{}_granularity-{}_start-{}_end-{}.csv\".format(symbol, granularity, start_date, end_date)\n",
    "dataset_train_name = \"./data/datasets/v1.0/symbol-{}_granularity-{}_start-{}_end-{}_train.csv\".format(symbol, granularity, start_date, end_date)\n",
    "dataset_valid_name = \"./data/datasets/v1.0/symbol-{}_granularity-{}_start-{}_end-{}_valid.csv\".format(symbol, granularity, start_date, end_date)\n",
    "\n",
    "\n",
    "# ログフォルダの生成\n",
    "log_dir = './logs/'\n",
    "datasets_dir = '../datasets/'\n",
    "\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(datasets_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# train data idx\n",
    "idx1 = 100\n",
    "idx2 = 5000\n",
    "\n",
    "# test data idx\n",
    "idx3 = 6000\n",
    "\n",
    "window_size = 100\n",
    "\n",
    "trade_fee = 0\n",
    "\n",
    "env_num = 10\n",
    "\n",
    "total_timesteps=100000\n",
    "\n",
    "# tb_log_name = \"PPO2_feat19\"\n",
    "tb_log_name = \"PPO2_feat57_100_lstm128\"\n",
    "\n",
    "DATASET_GET_FLAG = False\n",
    "\n",
    "n_steps = 128\n",
    "\n",
    "save_freq = 100\n",
    "eval_freq = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_ti = pd.read_csv(dataset_train_name, index_col=0)\n",
    "df_test_ti = pd.read_csv(dataset_valid_name, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# データ読み込み部分\n",
    "#\n",
    "def my_process_data(env):\n",
    "    start = env.frame_bound[0] - env.window_size\n",
    "    end = env.frame_bound[1]\n",
    "    prices = env.df.loc[:, 'Low'].to_numpy()[start:end]\n",
    "\n",
    "    # -----------------------------\n",
    "    # 特徴量生成\n",
    "    #\n",
    "    df_features = env.df.copy()\n",
    "    # df_features.drop(\"\")\n",
    "    ohlc_features = df_features.loc[:, :].to_numpy()[start:end]\n",
    "\n",
    "    #print(ohlc_features.shape)\n",
    "    #print(np.diff(ohlc_features, axis=0).shape)\n",
    "    diff1 = np.insert(np.diff(ohlc_features, axis=0), 0, 0, axis=0)\n",
    "    diff2 = np.insert(np.diff(diff1, axis=0), 0, 0, axis=0)\n",
    "    #print(diff)\n",
    "    #signal_features = env.df.loc[:, ['Close', 'Open', 'High', 'Low', 'Vol']].to_numpy()[start:end]\n",
    "\n",
    "    #signal_features = np.column_stack((ohlc_features, diff1, diff2))\n",
    "    signal_features = np.column_stack((ohlc_features, ))\n",
    "\n",
    "    print(\">>> signal_features.shape\")\n",
    "    print( signal_features.shape)\n",
    "    #print( signal_features.head(5))\n",
    "\n",
    "    return prices, signal_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# 環境クラス\n",
    "#\n",
    "class MyBTCEnv(ForexEnv):\n",
    "    _process_data = my_process_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5637"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train_ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> signal_features.shape\n",
      "(2417, 5)\n",
      ">>> signal_features.shape\n",
      "(5637, 5)\n",
      ">>> signal_features.shape\n",
      "(5637, 5)\n",
      ">>> signal_features.shape\n",
      "(5637, 5)\n",
      ">>> signal_features.shape\n",
      "(5637, 5)\n",
      ">>> signal_features.shape\n",
      "(5637, 5)\n",
      ">>> signal_features.shape\n",
      "(5637, 5)\n",
      ">>> signal_features.shape\n",
      "(5637, 5)\n",
      ">>> signal_features.shape\n",
      "(5637, 5)\n",
      ">>> signal_features.shape\n",
      "(5637, 5)\n",
      ">>> signal_features.shape\n",
      "(5637, 5)\n",
      ">>> signal_features.shape\n",
      "(2417, 5)\n"
     ]
    }
   ],
   "source": [
    "#####################################\n",
    "# 環境の生成\n",
    "#\n",
    "env_marker_train = lambda:  MyBTCEnv(df=df_train_ti, window_size=window_size, frame_bound=(window_size, len(df_train_ti)))\n",
    "env_marker_train.trade_fee = trade_fee\n",
    "\n",
    "env_marker_test = lambda:  MyBTCEnv(df=df_test_ti, window_size=window_size, frame_bound=(window_size, len(df_test_ti)))\n",
    "env_marker_test.trade_fee = trade_fee\n",
    "\n",
    "env_marker_test2 = env_marker_test()\n",
    "env_marker_test2.trade_fee = trade_fee\n",
    "\n",
    "env = DummyVecEnv([env_marker_train for _ in range(env_num)])\n",
    "#env = SubprocVecEnv([env_marker_train for i in range(env_num)])\n",
    "\n",
    "env_test = DummyVecEnv([env_marker_test for _ in range(1)])\n",
    "\n",
    "\n",
    "\n",
    "#env = Monitor(env, log_dir, allow_early_resets=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckpointCallback2(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model every `save_freq` steps\n",
    "\n",
    "    :param save_freq: (int)\n",
    "    :param save_path: (str) Path to the folder where the model will be saved.\n",
    "    :param name_prefix: (str) Common prefix to the saved models\n",
    "    \"\"\"\n",
    "    def __init__(self, save_freq: int, save_path: str, name_prefix='rl_model', verbose=0):\n",
    "        super(CheckpointCallback2, self).__init__(verbose)\n",
    "        self.save_freq = save_freq\n",
    "        self.save_path = save_path\n",
    "        self.name_prefix = name_prefix\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.save_freq == 0:\n",
    "            path = os.path.join(self.save_path, '{}_{:09d}_steps'.format(self.name_prefix, self.num_timesteps))\n",
    "            self.model.save(path)\n",
    "            if self.verbose > 1:\n",
    "                print(\"Saving model checkpoint to {}\".format(path))\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalCallback2(EventCallback):\n",
    "    \"\"\"\n",
    "    Callback for evaluating an agent.\n",
    "\n",
    "    :param eval_env: (Union[gym.Env, VecEnv]) The environment used for initialization\n",
    "    :param callback_on_new_best: (Optional[BaseCallback]) Callback to trigger\n",
    "        when there is a new best model according to the `mean_reward`\n",
    "    :param n_eval_episodes: (int) The number of episodes to test the agent\n",
    "    :param eval_freq: (int) Evaluate the agent every eval_freq call of the callback.\n",
    "    :param log_path: (str) Path to a folder where the evaluations (`evaluations.npz`)\n",
    "        will be saved. It will be updated at each evaluation.\n",
    "    :param best_model_save_path: (str) Path to a folder where the best model\n",
    "        according to performance on the eval env will be saved.\n",
    "    :param deterministic: (bool) Whether the evaluation should\n",
    "        use a stochastic or deterministic actions.\n",
    "    :param render: (bool) Whether to render or not the environment during evaluation\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "    def __init__(self, eval_env: Union[gym.Env, VecEnv],\n",
    "                 callback_on_new_best: Optional[BaseCallback] = None,\n",
    "                 n_eval_episodes: int = 5,\n",
    "                 eval_freq: int = 10000,\n",
    "                 log_path: str = None,\n",
    "                 best_model_save_path: str = None,\n",
    "                 deterministic: bool = True,\n",
    "                 render: bool = False,\n",
    "                 verbose: int = 1,\n",
    "                 name_prefix:str = None):\n",
    "      \n",
    "        super(EvalCallback2, self).__init__(callback_on_new_best, verbose=verbose)\n",
    "        self.n_eval_episodes = n_eval_episodes\n",
    "        self.eval_freq = eval_freq\n",
    "        self.best_mean_reward = -np.inf\n",
    "        self.last_mean_reward = -np.inf\n",
    "        self.deterministic = deterministic\n",
    "        self.render = render\n",
    "        self.name_prefix = name_prefix\n",
    "\n",
    "        # Convert to VecEnv for consistency\n",
    "        if not isinstance(eval_env, VecEnv):\n",
    "            eval_env = DummyVecEnv([lambda: eval_env])\n",
    "\n",
    "        assert eval_env.num_envs == 1, \"You must pass only one environment for evaluation\"\n",
    "\n",
    "        self.eval_env = eval_env\n",
    "        self.best_model_save_path = best_model_save_path\n",
    "        # Logs will be written in `evaluations.npz`\n",
    "        if log_path is not None:\n",
    "            log_path = os.path.join(log_path, 'evaluations')\n",
    "        self.log_path = log_path\n",
    "        self.evaluations_results = []\n",
    "        self.evaluations_timesteps = []\n",
    "        self.evaluations_length = []\n",
    "\n",
    "    def _init_callback(self):\n",
    "        # Does not work in some corner cases, where the wrapper is not the same\n",
    "        if not type(self.training_env) is type(self.eval_env):\n",
    "            warnings.warn(\"Training and eval env are not of the same type\"\n",
    "                          \"{} != {}\".format(self.training_env, self.eval_env))\n",
    "\n",
    "        # Create folders if needed\n",
    "        if self.best_model_save_path is not None:\n",
    "            os.makedirs(self.best_model_save_path, exist_ok=True)\n",
    "        if self.log_path is not None:\n",
    "            os.makedirs(os.path.dirname(self.log_path), exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "\n",
    "        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:\n",
    "            # Sync training and eval env if there is VecNormalize\n",
    "            sync_envs_normalization(self.training_env, self.eval_env)\n",
    "\n",
    "            episode_rewards, episode_lengths = evaluate_policy(self.model, self.eval_env,\n",
    "                                                               n_eval_episodes=self.n_eval_episodes,\n",
    "                                                               render=self.render,\n",
    "                                                               deterministic=self.deterministic,\n",
    "                                                               return_episode_rewards=True)\n",
    "\n",
    "            print(\">>> log_path >>>\")\n",
    "            self.log_path2 = self.log_path + \"-{:09d}\".format(self.num_timesteps)\n",
    "            print(self.log_path)\n",
    "            print(self.log_path2)\n",
    "\n",
    "            if self.log_path2 is not None:\n",
    "                self.evaluations_timesteps.append(self.num_timesteps)\n",
    "                self.evaluations_results.append(episode_rewards)\n",
    "                self.evaluations_length.append(episode_lengths)\n",
    "                np.savez(self.log_path2, timesteps=self.evaluations_timesteps,\n",
    "                         results=self.evaluations_results, ep_lengths=self.evaluations_length)\n",
    "            \n",
    "            print(\">>> episode_rewards >>>\")\n",
    "            print(episode_rewards)\n",
    "\n",
    "\n",
    "            mean_reward, std_reward = np.mean(episode_rewards), np.std(episode_rewards)\n",
    "            mean_ep_length, std_ep_length = np.mean(episode_lengths), np.std(episode_lengths)\n",
    "            # Keep track of the last evaluation, useful for classes that derive from this callback\n",
    "            self.last_mean_reward = mean_reward\n",
    "\n",
    "            if self.verbose > 0:\n",
    "                print(\"Eval num_timesteps={}, \"\n",
    "                      \"episode_reward={:.2f} +/- {:.2f}\".format(self.num_timesteps, mean_reward, std_reward))\n",
    "                print(\"Episode length: {:.2f} +/- {:.2f}\".format(mean_ep_length, std_ep_length))\n",
    "\n",
    "            if mean_reward > self.best_mean_reward:\n",
    "                if self.verbose > 0:\n",
    "                    print(\"New best mean reward!\")\n",
    "                if self.best_model_save_path is not None:\n",
    "                    self.model.save(os.path.join(self.best_model_save_path, 'best_model'))\n",
    "                self.best_mean_reward = mean_reward\n",
    "                # Trigger callback if needed\n",
    "                if self.callback is not None:\n",
    "                    return self._on_event()\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------\n",
    "# first train\n",
    "#\n",
    "policy_kwargs = dict(net_arch=[64*2, 'lstm', dict(vf=[128, 128, 128], pi=[64*2, 64*2])])\n",
    "#model = PPO2('MlpLstmPolicy', env, verbose=0, policy_kwargs=policy_kwargs, nminibatches=env_num, tensorboard_log=log_dir, n_steps=n_steps)\n",
    "\n",
    "# ------\n",
    "# resume\n",
    "#\n",
    "#model = PPO2.load(\"/content/drive/MyDrive/AIncome/AutoTrade13/logs/rl_model_90000_steps\")\n",
    "#model.set_env(env)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_episode_reward_logger2(rew_acc, rewards, masks, writer, steps, n_steps=128, train_num=None):\n",
    "    \"\"\"\n",
    "    calculates the cumulated episode reward, and prints to tensorflow log the output\n",
    "\n",
    "    :param rew_acc: (np.array float) the total running reward\n",
    "    :param rewards: (np.array float) the rewards\n",
    "    :param masks: (np.array bool) the end of episodes\n",
    "    :param writer: (TensorFlow Session.writer) the writer to log to\n",
    "    :param steps: (int) the current timestep\n",
    "    :return: (np.array float) the updated total running reward\n",
    "    :return: (np.array float) the updated total running reward\n",
    "    \"\"\"\n",
    "\n",
    "    #print(\">>>>> step : {}\".format(steps))\n",
    "    #print(\"rewards: {}\".format(rewards))\n",
    "\n",
    "    with tf.variable_scope(\"environment_info\", reuse=True):\n",
    "        for env_idx in range(rewards.shape[0]):\n",
    "            dones_idx = np.sort(np.argwhere(masks[env_idx]))\n",
    "\n",
    "            \"\"\"\n",
    "            print(\"masks    : {}\".format(masks))\n",
    "            print(\"dones_idx: {}\".format(dones_idx))\n",
    "            print(\"dones_idx: {}\".format(len(dones_idx)))\n",
    "            \"\"\"\n",
    "\n",
    "            if len(dones_idx) == 0:\n",
    "                rew_acc[env_idx] += sum(rewards[env_idx])\n",
    "            else:\n",
    "                rew_acc[env_idx] += sum(rewards[env_idx, :dones_idx[0, 0]])\n",
    "                summary = tf.Summary(value=[tf.Summary.Value(tag=\"episode_reward\", simple_value=rew_acc[env_idx])])\n",
    "               \n",
    "                \n",
    "\n",
    "                \n",
    "                stepA = int((int(steps/(train_num)) -1)*train_num + (env_idx+1)*(train_num/rewards.shape[0]))\n",
    "\n",
    "                \"\"\"\n",
    "                print(\"---=== step+   : {}\".format(steps + dones_idx[0, 0]))\n",
    "                print(\"--->>> step    : {}\".format(steps))\n",
    "                print(\"------ env_idx : {}\".format(env_idx))\n",
    "                print(\"---*** stepA   : {}\".format(stepA))\n",
    "                \"\"\"\n",
    "\n",
    "                writer.add_summary(summary, stepA)\n",
    "\n",
    "                for k in range(1, len(dones_idx[:, 0])):\n",
    "                    rew_acc[env_idx] = sum(rewards[env_idx, dones_idx[k - 1, 0]:dones_idx[k, 0]])\n",
    "                    summary = tf.Summary(value=[tf.Summary.Value(tag=\"episode_reward\", simple_value=rew_acc[env_idx])])\n",
    "                    #writer.add_summary(summary, steps + dones_idx[k, 0])\n",
    "                    writer.add_summary(summary, steps + dones_idx[k, 0])\n",
    "                    print(\"---*** step : {}\".format(steps + dones_idx[k, 0]))\n",
    "                rew_acc[env_idx] = sum(rewards[env_idx, dones_idx[-1, 0]:])\n",
    "\n",
    "    return rew_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from stable_baselines import logger\n",
    "from stable_baselines.common import explained_variance, ActorCriticRLModel, tf_util, SetVerbosity, TensorboardWriter\n",
    "from stable_baselines.common.runners import AbstractEnvRunner\n",
    "from stable_baselines.common.policies import ActorCriticPolicy, RecurrentActorCriticPolicy\n",
    "from stable_baselines.common.schedules import get_schedule_fn\n",
    "#from stable_baselines.common.tf_util import total_episode_reward_logger\n",
    "from stable_baselines.common.math_util import safe_mean\n",
    "\n",
    "\n",
    "class PPO2(ActorCriticRLModel):\n",
    "    \"\"\"\n",
    "    Proximal Policy Optimization algorithm (GPU version).\n",
    "    Paper: https://arxiv.org/abs/1707.06347\n",
    "\n",
    "    :param policy: (ActorCriticPolicy or str) The policy model to use (MlpPolicy, CnnPolicy, CnnLstmPolicy, ...)\n",
    "    :param env: (Gym environment or str) The environment to learn from (if registered in Gym, can be str)\n",
    "    :param gamma: (float) Discount factor\n",
    "    :param n_steps: (int) The number of steps to run for each environment per update\n",
    "        (i.e. batch size is n_steps * n_env where n_env is number of environment copies running in parallel)\n",
    "    :param ent_coef: (float) Entropy coefficient for the loss calculation\n",
    "    :param learning_rate: (float or callable) The learning rate, it can be a function\n",
    "    :param vf_coef: (float) Value function coefficient for the loss calculation\n",
    "    :param max_grad_norm: (float) The maximum value for the gradient clipping\n",
    "    :param lam: (float) Factor for trade-off of bias vs variance for Generalized Advantage Estimator\n",
    "    :param nminibatches: (int) Number of training minibatches per update. For recurrent policies,\n",
    "        the number of environments run in parallel should be a multiple of nminibatches.\n",
    "    :param noptepochs: (int) Number of epoch when optimizing the surrogate\n",
    "    :param cliprange: (float or callable) Clipping parameter, it can be a function\n",
    "    :param cliprange_vf: (float or callable) Clipping parameter for the value function, it can be a function.\n",
    "        This is a parameter specific to the OpenAI implementation. If None is passed (default),\n",
    "        then `cliprange` (that is used for the policy) will be used.\n",
    "        IMPORTANT: this clipping depends on the reward scaling.\n",
    "        To deactivate value function clipping (and recover the original PPO implementation),\n",
    "        you have to pass a negative value (e.g. -1).\n",
    "    :param verbose: (int) the verbosity level: 0 none, 1 training information, 2 tensorflow debug\n",
    "    :param tensorboard_log: (str) the log location for tensorboard (if None, no logging)\n",
    "    :param _init_setup_model: (bool) Whether or not to build the network at the creation of the instance\n",
    "    :param policy_kwargs: (dict) additional arguments to be passed to the policy on creation\n",
    "    :param full_tensorboard_log: (bool) enable additional logging when using tensorboard\n",
    "        WARNING: this logging can take a lot of space quickly\n",
    "    :param seed: (int) Seed for the pseudo-random generators (python, numpy, tensorflow).\n",
    "        If None (default), use random seed. Note that if you want completely deterministic\n",
    "        results, you must set `n_cpu_tf_sess` to 1.\n",
    "    :param n_cpu_tf_sess: (int) The number of threads for TensorFlow operations\n",
    "        If None, the number of cpu of the current machine will be used.\n",
    "    \"\"\"\n",
    "    def __init__(self, policy, env, gamma=0.99, n_steps=128, ent_coef=0.01, learning_rate=2.5e-4, vf_coef=0.5,\n",
    "                 max_grad_norm=0.5, lam=0.95, nminibatches=4, noptepochs=4, cliprange=0.2, cliprange_vf=None,\n",
    "                 verbose=0, tensorboard_log=None, _init_setup_model=True, policy_kwargs=None,\n",
    "                 full_tensorboard_log=False, seed=None, n_cpu_tf_sess=None, train_num=None):\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.cliprange = cliprange\n",
    "        self.cliprange_vf = cliprange_vf\n",
    "        self.n_steps = n_steps\n",
    "        self.ent_coef = ent_coef\n",
    "        self.vf_coef = vf_coef\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.nminibatches = nminibatches\n",
    "        self.noptepochs = noptepochs\n",
    "        self.tensorboard_log = tensorboard_log\n",
    "        self.full_tensorboard_log = full_tensorboard_log\n",
    "\n",
    "        self.action_ph = None\n",
    "        self.advs_ph = None\n",
    "        self.rewards_ph = None\n",
    "        self.old_neglog_pac_ph = None\n",
    "        self.old_vpred_ph = None\n",
    "        self.learning_rate_ph = None\n",
    "        self.clip_range_ph = None\n",
    "        self.entropy = None\n",
    "        self.vf_loss = None\n",
    "        self.pg_loss = None\n",
    "        self.approxkl = None\n",
    "        self.clipfrac = None\n",
    "        self._train = None\n",
    "        self.loss_names = None\n",
    "        self.train_model = None\n",
    "        self.act_model = None\n",
    "        self.value = None\n",
    "        self.n_batch = None\n",
    "        self.summary = None\n",
    "        self.p_id    = 0\n",
    "        self.train_num = train_num\n",
    "\n",
    "        super().__init__(policy=policy, env=env, verbose=verbose, requires_vec_env=True,\n",
    "                         _init_setup_model=_init_setup_model, policy_kwargs=policy_kwargs,\n",
    "                         seed=seed, n_cpu_tf_sess=n_cpu_tf_sess)\n",
    "\n",
    "        if _init_setup_model:\n",
    "            self.setup_model()\n",
    "\n",
    "    def _make_runner(self):\n",
    "        return Runner(env=self.env, model=self, n_steps=self.n_steps,\n",
    "                      gamma=self.gamma, lam=self.lam)\n",
    "\n",
    "    def _get_pretrain_placeholders(self):\n",
    "        policy = self.act_model\n",
    "        if isinstance(self.action_space, gym.spaces.Discrete):\n",
    "            return policy.obs_ph, self.action_ph, policy.policy\n",
    "        return policy.obs_ph, self.action_ph, policy.deterministic_action\n",
    "\n",
    "    def setup_model(self):\n",
    "        with SetVerbosity(self.verbose):\n",
    "\n",
    "            assert issubclass(self.policy, ActorCriticPolicy), \"Error: the input policy for the PPO2 model must be \" \\\n",
    "                                                               \"an instance of common.policies.ActorCriticPolicy.\"\n",
    "\n",
    "            self.n_batch = self.n_envs * self.n_steps\n",
    "\n",
    "            self.graph = tf.Graph()\n",
    "            with self.graph.as_default():\n",
    "                self.set_random_seed(self.seed)\n",
    "                self.sess = tf_util.make_session(num_cpu=self.n_cpu_tf_sess, graph=self.graph)\n",
    "\n",
    "                n_batch_step = None\n",
    "                n_batch_train = None\n",
    "                if issubclass(self.policy, RecurrentActorCriticPolicy):\n",
    "                    assert self.n_envs % self.nminibatches == 0, \"For recurrent policies, \"\\\n",
    "                        \"the number of environments run in parallel should be a multiple of nminibatches.\"\n",
    "                    n_batch_step = self.n_envs\n",
    "                    n_batch_train = self.n_batch // self.nminibatches\n",
    "\n",
    "                act_model = self.policy(self.sess, self.observation_space, self.action_space, self.n_envs, 1,\n",
    "                                        n_batch_step, reuse=False, **self.policy_kwargs)\n",
    "                with tf.variable_scope(\"train_model\", reuse=True,\n",
    "                                       custom_getter=tf_util.outer_scope_getter(\"train_model\")):\n",
    "                    train_model = self.policy(self.sess, self.observation_space, self.action_space,\n",
    "                                              self.n_envs // self.nminibatches, self.n_steps, n_batch_train,\n",
    "                                              reuse=True, **self.policy_kwargs)\n",
    "\n",
    "                with tf.variable_scope(\"loss\", reuse=False):\n",
    "                    self.action_ph = train_model.pdtype.sample_placeholder([None], name=\"action_ph\")\n",
    "                    self.advs_ph = tf.placeholder(tf.float32, [None], name=\"advs_ph\")\n",
    "                    self.rewards_ph = tf.placeholder(tf.float32, [None], name=\"rewards_ph\")\n",
    "                    self.old_neglog_pac_ph = tf.placeholder(tf.float32, [None], name=\"old_neglog_pac_ph\")\n",
    "                    self.old_vpred_ph = tf.placeholder(tf.float32, [None], name=\"old_vpred_ph\")\n",
    "                    self.learning_rate_ph = tf.placeholder(tf.float32, [], name=\"learning_rate_ph\")\n",
    "                    self.clip_range_ph = tf.placeholder(tf.float32, [], name=\"clip_range_ph\")\n",
    "\n",
    "                    neglogpac = train_model.proba_distribution.neglogp(self.action_ph)\n",
    "                    self.entropy = tf.reduce_mean(train_model.proba_distribution.entropy())\n",
    "\n",
    "                    vpred = train_model.value_flat\n",
    "\n",
    "                    # Value function clipping: not present in the original PPO\n",
    "                    if self.cliprange_vf is None:\n",
    "                        # Default behavior (legacy from OpenAI baselines):\n",
    "                        # use the same clipping as for the policy\n",
    "                        self.clip_range_vf_ph = self.clip_range_ph\n",
    "                        self.cliprange_vf = self.cliprange\n",
    "                    elif isinstance(self.cliprange_vf, (float, int)) and self.cliprange_vf < 0:\n",
    "                        # Original PPO implementation: no value function clipping\n",
    "                        self.clip_range_vf_ph = None\n",
    "                    else:\n",
    "                        # Last possible behavior: clipping range\n",
    "                        # specific to the value function\n",
    "                        self.clip_range_vf_ph = tf.placeholder(tf.float32, [], name=\"clip_range_vf_ph\")\n",
    "\n",
    "                    if self.clip_range_vf_ph is None:\n",
    "                        # No clipping\n",
    "                        vpred_clipped = train_model.value_flat\n",
    "                    else:\n",
    "                        # Clip the different between old and new value\n",
    "                        # NOTE: this depends on the reward scaling\n",
    "                        vpred_clipped = self.old_vpred_ph + \\\n",
    "                            tf.clip_by_value(train_model.value_flat - self.old_vpred_ph,\n",
    "                                             - self.clip_range_vf_ph, self.clip_range_vf_ph)\n",
    "\n",
    "                    vf_losses1 = tf.square(vpred - self.rewards_ph)\n",
    "                    vf_losses2 = tf.square(vpred_clipped - self.rewards_ph)\n",
    "                    self.vf_loss = .5 * tf.reduce_mean(tf.maximum(vf_losses1, vf_losses2))\n",
    "\n",
    "                    ratio = tf.exp(self.old_neglog_pac_ph - neglogpac)\n",
    "                    pg_losses = -self.advs_ph * ratio\n",
    "                    pg_losses2 = -self.advs_ph * tf.clip_by_value(ratio, 1.0 - self.clip_range_ph, 1.0 +\n",
    "                                                                  self.clip_range_ph)\n",
    "                    self.pg_loss = tf.reduce_mean(tf.maximum(pg_losses, pg_losses2))\n",
    "                    self.approxkl = .5 * tf.reduce_mean(tf.square(neglogpac - self.old_neglog_pac_ph))\n",
    "                    self.clipfrac = tf.reduce_mean(tf.cast(tf.greater(tf.abs(ratio - 1.0),\n",
    "                                                                      self.clip_range_ph), tf.float32))\n",
    "                    loss = self.pg_loss - self.entropy * self.ent_coef + self.vf_loss * self.vf_coef\n",
    "\n",
    "                    tf.summary.scalar('entropy_loss', self.entropy)\n",
    "                    \"\"\"\n",
    "                    print(\"------------------------------\")\n",
    "                    print(\" self.entropy \")\n",
    "                    print(self.entropy)\n",
    "                    \"\"\"\n",
    "\n",
    "                    tf.summary.scalar('policy_gradient_loss', self.pg_loss)\n",
    "                    tf.summary.scalar('value_function_loss', self.vf_loss)\n",
    "                    tf.summary.scalar('approximate_kullback-leibler', self.approxkl)\n",
    "                    tf.summary.scalar('clip_factor', self.clipfrac)\n",
    "                    tf.summary.scalar('loss', loss)\n",
    "\n",
    "                    with tf.variable_scope('model'):\n",
    "                        self.params = tf.trainable_variables()\n",
    "                        if self.full_tensorboard_log:\n",
    "                            for var in self.params:\n",
    "                                tf.summary.histogram(var.name, var)\n",
    "                    grads = tf.gradients(loss, self.params)\n",
    "                    if self.max_grad_norm is not None:\n",
    "                        grads, _grad_norm = tf.clip_by_global_norm(grads, self.max_grad_norm)\n",
    "                    grads = list(zip(grads, self.params))\n",
    "                trainer = tf.train.AdamOptimizer(learning_rate=self.learning_rate_ph, epsilon=1e-5)\n",
    "                self._train = trainer.apply_gradients(grads)\n",
    "\n",
    "                self.loss_names = ['policy_loss', 'value_loss', 'policy_entropy', 'approxkl', 'clipfrac']\n",
    "\n",
    "                with tf.variable_scope(\"input_info\", reuse=False):\n",
    "                    tf.summary.scalar('discounted_rewards', tf.reduce_mean(self.rewards_ph))\n",
    "                    tf.summary.scalar('learning_rate', tf.reduce_mean(self.learning_rate_ph))\n",
    "                    tf.summary.scalar('advantage', tf.reduce_mean(self.advs_ph))\n",
    "                    tf.summary.scalar('clip_range', tf.reduce_mean(self.clip_range_ph))\n",
    "                    if self.clip_range_vf_ph is not None:\n",
    "                        tf.summary.scalar('clip_range_vf', tf.reduce_mean(self.clip_range_vf_ph))\n",
    "\n",
    "                    tf.summary.scalar('old_neglog_action_probability', tf.reduce_mean(self.old_neglog_pac_ph))\n",
    "                    tf.summary.scalar('old_value_pred', tf.reduce_mean(self.old_vpred_ph))\n",
    "\n",
    "                    if self.full_tensorboard_log:\n",
    "                        tf.summary.histogram('discounted_rewards', self.rewards_ph)\n",
    "                        tf.summary.histogram('learning_rate', self.learning_rate_ph)\n",
    "                        tf.summary.histogram('advantage', self.advs_ph)\n",
    "                        tf.summary.histogram('clip_range', self.clip_range_ph)\n",
    "                        tf.summary.histogram('old_neglog_action_probability', self.old_neglog_pac_ph)\n",
    "                        tf.summary.histogram('old_value_pred', self.old_vpred_ph)\n",
    "                        if tf_util.is_image(self.observation_space):\n",
    "                            tf.summary.image('observation', train_model.obs_ph)\n",
    "                        else:\n",
    "                            tf.summary.histogram('observation', train_model.obs_ph)\n",
    "\n",
    "                self.train_model = train_model\n",
    "                self.act_model = act_model\n",
    "                self.step = act_model.step\n",
    "                self.proba_step = act_model.proba_step\n",
    "                self.value = act_model.value\n",
    "                self.initial_state = act_model.initial_state\n",
    "                tf.global_variables_initializer().run(session=self.sess)  # pylint: disable=E1101\n",
    "\n",
    "                self.summary = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "    def _train_step(self, learning_rate, cliprange, obs, returns, masks, actions, values, neglogpacs, update,\n",
    "                    writer, states=None, cliprange_vf=None):\n",
    "        \"\"\"\n",
    "        Training of PPO2 Algorithm\n",
    "\n",
    "        :param learning_rate: (float) learning rate\n",
    "        :param cliprange: (float) Clipping factor\n",
    "        :param obs: (np.ndarray) The current observation of the environment\n",
    "        :param returns: (np.ndarray) the rewards\n",
    "        :param masks: (np.ndarray) The last masks for done episodes (used in recurent policies)\n",
    "        :param actions: (np.ndarray) the actions\n",
    "        :param values: (np.ndarray) the values\n",
    "        :param neglogpacs: (np.ndarray) Negative Log-likelihood probability of Actions\n",
    "        :param update: (int) the current step iteration\n",
    "        :param writer: (TensorFlow Summary.writer) the writer for tensorboard\n",
    "        :param states: (np.ndarray) For recurrent policies, the internal state of the recurrent model\n",
    "        :return: policy gradient loss, value function loss, policy entropy,\n",
    "                approximation of kl divergence, updated clipping range, training update operation\n",
    "        :param cliprange_vf: (float) Clipping factor for the value function\n",
    "        \"\"\"\n",
    "        advs = returns - values\n",
    "        advs = (advs - advs.mean()) / (advs.std() + 1e-8)\n",
    "        td_map = {self.train_model.obs_ph: obs, self.action_ph: actions,\n",
    "                  self.advs_ph: advs, self.rewards_ph: returns,\n",
    "                  self.learning_rate_ph: learning_rate, self.clip_range_ph: cliprange,\n",
    "                  self.old_neglog_pac_ph: neglogpacs, self.old_vpred_ph: values}\n",
    "        if states is not None:\n",
    "            td_map[self.train_model.states_ph] = states\n",
    "            td_map[self.train_model.dones_ph] = masks\n",
    "\n",
    "        if cliprange_vf is not None and cliprange_vf >= 0:\n",
    "            td_map[self.clip_range_vf_ph] = cliprange_vf\n",
    "\n",
    "        if states is None:\n",
    "            update_fac = max(self.n_batch // self.nminibatches // self.noptepochs, 1)\n",
    "        else:\n",
    "            update_fac = max(self.n_batch // self.nminibatches // self.noptepochs // self.n_steps, 1)\n",
    "\n",
    "        if writer is not None:\n",
    "            # run loss backprop with summary, but once every 10 runs save the metadata (memory, compute time, ...)\n",
    "            if self.full_tensorboard_log and (1 + update) % 10 == 0:\n",
    "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                run_metadata = tf.RunMetadata()\n",
    "                summary, policy_loss, value_loss, policy_entropy, approxkl, clipfrac, _ = self.sess.run(\n",
    "                    [self.summary, self.pg_loss, self.vf_loss, self.entropy, self.approxkl, self.clipfrac, self._train],\n",
    "                    td_map, options=run_options, run_metadata=run_metadata)\n",
    "                #print(\"update---->{}\".format(update))\n",
    "                writer.add_run_metadata(run_metadata, 'step%d' % (update * update_fac))\n",
    "            else:\n",
    "                summary, policy_loss, value_loss, policy_entropy, approxkl, clipfrac, _ = self.sess.run(\n",
    "                    [self.summary, self.pg_loss, self.vf_loss, self.entropy, self.approxkl, self.clipfrac, self._train],\n",
    "                    td_map)\n",
    "            writer.add_summary(summary, (update * update_fac))\n",
    "            \"\"\"\n",
    "            print(\">>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "            print(\"update             : {}\".format(update))\n",
    "            print(\"update_fac         : {}\".format(update_fac))\n",
    "            print(\"update*update_fac  : {}\".format(update * update_fac))\n",
    "            print(\"self.entropy:      : {}\".format(self.entropy))\n",
    "            \"\"\"\n",
    "        else:\n",
    "            policy_loss, value_loss, policy_entropy, approxkl, clipfrac, _ = self.sess.run(\n",
    "                [self.pg_loss, self.vf_loss, self.entropy, self.approxkl, self.clipfrac, self._train], td_map)\n",
    "\n",
    "        return policy_loss, value_loss, policy_entropy, approxkl, clipfrac\n",
    "\n",
    "    def learn(self, total_timesteps, callback=None, log_interval=1, tb_log_name=\"PPO2\",\n",
    "              reset_num_timesteps=True):\n",
    "        # Transform to callable if needed\n",
    "        self.learning_rate = get_schedule_fn(self.learning_rate)\n",
    "        self.cliprange = get_schedule_fn(self.cliprange)\n",
    "        cliprange_vf = get_schedule_fn(self.cliprange_vf)\n",
    "\n",
    "        new_tb_log = self._init_num_timesteps(reset_num_timesteps)\n",
    "        callback = self._init_callback(callback)\n",
    "\n",
    "        with SetVerbosity(self.verbose), TensorboardWriter(self.graph, self.tensorboard_log, tb_log_name, new_tb_log) \\\n",
    "                as writer:\n",
    "            self._setup_learn()\n",
    "\n",
    "            t_first_start = time.time()\n",
    "            #n_updates = total_timesteps // self.n_batch\n",
    "            n_updates = total_timesteps\n",
    "\n",
    "            callback.on_training_start(locals(), globals())\n",
    "\n",
    "            \"\"\"\n",
    "            print(\"---------------------------------\")\n",
    "            print(\"n_updates + 1 : {}\".format(n_updates + 1))\n",
    "            \"\"\"\n",
    "\n",
    "            for update in tqdm(range(1, n_updates + 1)):\n",
    "                assert self.n_batch % self.nminibatches == 0, (\"The number of minibatches (`nminibatches`) \"\n",
    "                                                               \"is not a factor of the total number of samples \"\n",
    "                                                               \"collected per rollout (`n_batch`), \"\n",
    "                                                               \"some samples won't be used.\"\n",
    "                                                               )\n",
    "                batch_size = self.n_batch // self.nminibatches\n",
    "                t_start = time.time()\n",
    "                frac = 1.0 - (update - 1.0) / n_updates\n",
    "                lr_now = self.learning_rate(frac)\n",
    "                cliprange_now = self.cliprange(frac)\n",
    "                cliprange_vf_now = cliprange_vf(frac)\n",
    "\n",
    "                callback.on_rollout_start()\n",
    "                # true_reward is the reward without discount\n",
    "                rollout = self.runner.run(callback)\n",
    "                # Unpack\n",
    "                obs, returns, masks, actions, values, neglogpacs, states, ep_infos, true_reward = rollout\n",
    "\n",
    "                callback.on_rollout_end()\n",
    "\n",
    "                # Early stopping due to the callback\n",
    "                if not self.runner.continue_training:\n",
    "                    break\n",
    "\n",
    "                self.ep_info_buf.extend(ep_infos)\n",
    "                mb_loss_vals = []\n",
    "                if states is None:  # nonrecurrent version\n",
    "                    update_fac = max(self.n_batch // self.nminibatches // self.noptepochs, 1)\n",
    "                    inds = np.arange(self.n_batch)\n",
    "                    for epoch_num in range(self.noptepochs):\n",
    "                        np.random.shuffle(inds)\n",
    "                        for start in range(0, self.n_batch, batch_size):\n",
    "                            timestep = self.num_timesteps // update_fac + ((epoch_num *\n",
    "                                                                            self.n_batch + start) // batch_size)\n",
    "                            end = start + batch_size\n",
    "                            mbinds = inds[start:end]\n",
    "                            slices = (arr[mbinds] for arr in (obs, returns, masks, actions, values, neglogpacs))\n",
    "                            mb_loss_vals.append(self._train_step(lr_now, cliprange_now, *slices, writer=writer,\n",
    "                                                                 update=timestep, cliprange_vf=cliprange_vf_now))\n",
    "                else:  # recurrent version\n",
    "                    update_fac = max(self.n_batch // self.nminibatches // self.noptepochs // self.n_steps, 1)\n",
    "                    assert self.n_envs % self.nminibatches == 0\n",
    "                    env_indices = np.arange(self.n_envs)\n",
    "                    flat_indices = np.arange(self.n_envs * self.n_steps).reshape(self.n_envs, self.n_steps)\n",
    "                    envs_per_batch = batch_size // self.n_steps\n",
    "                    for epoch_num in range(1):\n",
    "                        np.random.shuffle(env_indices)\n",
    "                        for start in range(0, self.n_envs, envs_per_batch):\n",
    "                            timestep = self.num_timesteps // update_fac + ((epoch_num *\n",
    "                                                                            self.n_envs + start) // envs_per_batch)\n",
    "                            \n",
    "\n",
    "                            \"\"\"\n",
    "                            print(\">>>> timestep          :{}\".format(timestep))\n",
    "                            print(\">>>> self.num_timesteps:{}\".format(self.num_timesteps))\n",
    "                            print(\">>>> update_fac        :{}\".format(update_fac))\n",
    "                            print(\">>>> epoch_num         :{}\".format(epoch_num))\n",
    "                            print(\">>>> self.n_envs       :{}\".format(self.n_envs))\n",
    "                            print(\">>>> start             :{}\".format(start))\n",
    "                            print(\">>>> envs_per_batch    :{}\".format(envs_per_batch))\n",
    "                            print(\">>>> self.noptepochs   :{}\".format(self.noptepochs))\n",
    "                            print(\">>>>>>>> self.num_timesteps // update_fac + ((epoch_num * self.n_envs + start) // envs_per_batch)\")\n",
    "                            \"\"\"\n",
    "\n",
    "                            end = start + envs_per_batch\n",
    "                            mb_env_inds = env_indices[start:end]\n",
    "                            mb_flat_inds = flat_indices[mb_env_inds].ravel()\n",
    "                            slices = (arr[mb_flat_inds] for arr in (obs, returns, masks, actions, values, neglogpacs))\n",
    "                            mb_states = states[mb_env_inds]\n",
    "                            mb_loss_vals.append(self._train_step(lr_now, cliprange_now, *slices, update=timestep,\n",
    "                                                                 writer=writer, states=mb_states,\n",
    "                                                                 cliprange_vf=cliprange_vf_now))\n",
    "\n",
    "                loss_vals = np.mean(mb_loss_vals, axis=0)\n",
    "                t_now = time.time()\n",
    "                fps = int(self.n_batch / (t_now - t_start))\n",
    "\n",
    "                if writer is not None:\n",
    "                    \n",
    "                    #print(\">>>>>>> writer >>>>>>> \")\n",
    "                    \n",
    "\n",
    "                    total_episode_reward_logger2(self.episode_reward,\n",
    "                                                true_reward.reshape((self.n_envs, self.n_steps)),\n",
    "                                                masks.reshape((self.n_envs, self.n_steps)),\n",
    "                                                writer, self.num_timesteps, n_steps=self.n_steps, train_num=self.train_num)\n",
    "\n",
    "                if self.verbose >= 1 and (update % log_interval == 0 or update == 1):\n",
    "                    explained_var = explained_variance(values, returns)\n",
    "                    logger.logkv(\"serial_timesteps\", update * self.n_steps)\n",
    "                    logger.logkv(\"n_updates\", update)\n",
    "                    logger.logkv(\"total_timesteps\", self.num_timesteps)\n",
    "                    logger.logkv(\"fps\", fps)\n",
    "                    logger.logkv(\"explained_variance\", float(explained_var))\n",
    "                    if len(self.ep_info_buf) > 0 and len(self.ep_info_buf[0]) > 0:\n",
    "                        logger.logkv('ep_reward_mean', safe_mean([ep_info['r'] for ep_info in self.ep_info_buf]))\n",
    "                        logger.logkv('ep_len_mean', safe_mean([ep_info['l'] for ep_info in self.ep_info_buf]))\n",
    "                    logger.logkv('time_elapsed', t_start - t_first_start)\n",
    "                    for (loss_val, loss_name) in zip(loss_vals, self.loss_names):\n",
    "                        logger.logkv(loss_name, loss_val)\n",
    "                    logger.dumpkvs()\n",
    "\n",
    "            callback.on_training_end()\n",
    "            return self\n",
    "\n",
    "\n",
    "    def save(self, save_path, cloudpickle=False):\n",
    "        data = {\n",
    "            \"gamma\": self.gamma,\n",
    "            \"n_steps\": self.n_steps,\n",
    "            \"vf_coef\": self.vf_coef,\n",
    "            \"ent_coef\": self.ent_coef,\n",
    "            \"max_grad_norm\": self.max_grad_norm,\n",
    "            \"learning_rate\": self.learning_rate,\n",
    "            \"lam\": self.lam,\n",
    "            \"nminibatches\": self.nminibatches,\n",
    "            \"noptepochs\": self.noptepochs,\n",
    "            \"cliprange\": self.cliprange,\n",
    "            \"cliprange_vf\": self.cliprange_vf,\n",
    "            \"verbose\": self.verbose,\n",
    "            \"policy\": self.policy,\n",
    "            \"observation_space\": self.observation_space,\n",
    "            \"action_space\": self.action_space,\n",
    "            \"n_envs\": self.n_envs,\n",
    "            \"n_cpu_tf_sess\": self.n_cpu_tf_sess,\n",
    "            \"seed\": self.seed,\n",
    "            \"_vectorize_action\": self._vectorize_action,\n",
    "            \"policy_kwargs\": self.policy_kwargs\n",
    "        }\n",
    "\n",
    "        params_to_save = self.get_parameters()\n",
    "\n",
    "        self._save_to_file(save_path, data=data, params=params_to_save, cloudpickle=cloudpickle)\n",
    "\n",
    "\n",
    "\n",
    "class Runner(AbstractEnvRunner):\n",
    "    def __init__(self, *, env, model, n_steps, gamma, lam):\n",
    "        \"\"\"\n",
    "        A runner to learn the policy of an environment for a model\n",
    "\n",
    "        :param env: (Gym environment) The environment to learn from\n",
    "        :param model: (Model) The model to learn\n",
    "        :param n_steps: (int) The number of steps to run for each environment\n",
    "        :param gamma: (float) Discount factor\n",
    "        :param lam: (float) Factor for trade-off of bias vs variance for Generalized Advantage Estimator\n",
    "        \"\"\"\n",
    "        super().__init__(env=env, model=model, n_steps=n_steps)\n",
    "        self.lam = lam\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def _run(self):\n",
    "        \"\"\"\n",
    "        Run a learning step of the model\n",
    "\n",
    "        :return:\n",
    "            - observations: (np.ndarray) the observations\n",
    "            - rewards: (np.ndarray) the rewards\n",
    "            - masks: (numpy bool) whether an episode is over or not\n",
    "            - actions: (np.ndarray) the actions\n",
    "            - values: (np.ndarray) the value function output\n",
    "            - negative log probabilities: (np.ndarray)\n",
    "            - states: (np.ndarray) the internal states of the recurrent policies\n",
    "            - infos: (dict) the extra information of the model\n",
    "        \"\"\"\n",
    "        # mb stands for minibatch\n",
    "        mb_obs, mb_rewards, mb_actions, mb_values, mb_dones, mb_neglogpacs = [], [], [], [], [], []\n",
    "        mb_states = self.states\n",
    "        ep_infos = []\n",
    "\n",
    "        #print(\"-----------------------------\")\n",
    "        #print(\"self.model.num_timesteps : {}\".format(self.model.num_timesteps))\n",
    "        self.model.num_timesteps += self.n_envs       \n",
    "        ##print(\"self.model.num_timesteps : {}\".format(self.model.num_timesteps))\n",
    "        #print(\"self.n_envs : {}\".format(self.n_envs))\n",
    "\n",
    "        #self.model.num_timesteps += 1\n",
    "        #self.model.num_timesteps += 4\n",
    "\n",
    "        for _ in range(self.n_steps):\n",
    "            actions, values, self.states, neglogpacs = self.model.step(self.obs, self.states, self.dones)  # pytype: disable=attribute-error\n",
    "            mb_obs.append(self.obs.copy())\n",
    "            mb_actions.append(actions)\n",
    "            mb_values.append(values)\n",
    "            mb_neglogpacs.append(neglogpacs)\n",
    "            mb_dones.append(self.dones)\n",
    "            clipped_actions = actions\n",
    "            # Clip the actions to avoid out of bound error\n",
    "            if isinstance(self.env.action_space, gym.spaces.Box):\n",
    "                clipped_actions = np.clip(actions, self.env.action_space.low, self.env.action_space.high)\n",
    "            self.obs[:], rewards, self.dones, infos = self.env.step(clipped_actions)\n",
    "\n",
    "            #print(\"**** num_timesteps before : {}\".format(self.model.num_timesteps))\n",
    "            #self.model.num_timesteps += self.n_envs\n",
    "            #print(\"**** num_timesteps after  : {}\".format(self.model.num_timesteps))\n",
    "\n",
    "            \"\"\"\n",
    "            if self.callback is not None:\n",
    "                # Abort training early\n",
    "                self.callback.update_locals(locals())\n",
    "                if self.callback.on_step() is False:\n",
    "                    self.continue_training = False\n",
    "                    # Return dummy values\n",
    "                    return [None] * 9\n",
    "            \"\"\"\n",
    "            \n",
    "            for info in infos:\n",
    "                maybe_ep_info = info.get('episode')\n",
    "                if maybe_ep_info is not None:\n",
    "                    ep_infos.append(maybe_ep_info)\n",
    "            mb_rewards.append(rewards)\n",
    "        # batch of steps to batch of rollouts\n",
    "\n",
    "        if self.callback is not None:\n",
    "            # Abort training early\n",
    "            self.callback.update_locals(locals())\n",
    "            if self.callback.on_step() is False:\n",
    "                self.continue_training = False\n",
    "                # Return dummy values\n",
    "                return [None] * 9\n",
    "\n",
    "\n",
    "        mb_obs = np.asarray(mb_obs, dtype=self.obs.dtype)\n",
    "        mb_rewards = np.asarray(mb_rewards, dtype=np.float32)\n",
    "        mb_actions = np.asarray(mb_actions)\n",
    "        mb_values = np.asarray(mb_values, dtype=np.float32)\n",
    "        mb_neglogpacs = np.asarray(mb_neglogpacs, dtype=np.float32)\n",
    "        mb_dones = np.asarray(mb_dones, dtype=np.bool)\n",
    "        last_values = self.model.value(self.obs, self.states, self.dones)  # pytype: disable=attribute-error\n",
    "        # discount/bootstrap off value fn\n",
    "        mb_advs = np.zeros_like(mb_rewards)\n",
    "        true_reward = np.copy(mb_rewards)\n",
    "        last_gae_lam = 0\n",
    "        for step in reversed(range(self.n_steps)):\n",
    "            if step == self.n_steps - 1:\n",
    "                nextnonterminal = 1.0 - self.dones\n",
    "                nextvalues = last_values\n",
    "            else:\n",
    "                nextnonterminal = 1.0 - mb_dones[step + 1]\n",
    "                nextvalues = mb_values[step + 1]\n",
    "            delta = mb_rewards[step] + self.gamma * nextvalues * nextnonterminal - mb_values[step]\n",
    "            mb_advs[step] = last_gae_lam = delta + self.gamma * self.lam * nextnonterminal * last_gae_lam\n",
    "        mb_returns = mb_advs + mb_values\n",
    "\n",
    "        mb_obs, mb_returns, mb_dones, mb_actions, mb_values, mb_neglogpacs, true_reward = \\\n",
    "            map(swap_and_flatten, (mb_obs, mb_returns, mb_dones, mb_actions, mb_values, mb_neglogpacs, true_reward))\n",
    "\n",
    "        return mb_obs, mb_returns, mb_dones, mb_actions, mb_values, mb_neglogpacs, mb_states, ep_infos, true_reward\n",
    "\n",
    "\n",
    "# obs, returns, masks, actions, values, neglogpacs, states = runner.run()\n",
    "def swap_and_flatten(arr):\n",
    "    \"\"\"\n",
    "    swap and then flatten axes 0 and 1\n",
    "\n",
    "    :param arr: (np.ndarray)\n",
    "    :return: (np.ndarray)\n",
    "    \"\"\"\n",
    "    shape = arr.shape\n",
    "    return arr.swapaxes(0, 1).reshape(shape[0] * shape[1], *shape[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_FLAG = False\n",
    "resume_idx    = 0\n",
    "train_num     = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_num : 1\n"
     ]
    }
   ],
   "source": [
    "mean_reward_list = []\n",
    "\n",
    "print(\"train_num : {}\".format(train_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# ---------\n",
    "# load save model\n",
    "model_list = sorted(glob.glob(\"logs/*.zip\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_model_param():\n",
    "    # ---------\n",
    "    # load save model\n",
    "    model_list = sorted(glob.glob(\"logs/*.zip\"))\n",
    "    resume_FLAG = False\n",
    "\n",
    "    if(len(model_list)>0):\n",
    "        model_latest = model_list[-1].split(\"/\")[-1].split(\".zip\")[0].split(\"_step\")[0]\n",
    "        resume_FLAG = True\n",
    "        return model_latest.split(\"_\")[0], int(model_latest.split(\"_\")[1]), int(model_latest.split(\"_\")[2]), resume_FLAG\n",
    "\n",
    "    return None, None, None, resume_FLAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f038cf5f9e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f038cf5f9e8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f038cf5f9e8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f038cf5f9e8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f038cd97d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f038cd97d68>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f038cd97d68>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f038cd97d68>>: AttributeError: module 'gast' has no attribute 'Index'\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Create Env & Model\n",
    "#\n",
    "# env = gym.make('LunarLander-v2')\n",
    "#model = DQN('MlpPolicy', env, learning_rate=1e-3, prioritized_replay=True, verbose=0, tensorboard_log=log_dir, full_tensorboard_log=True)\n",
    "model = PPO2('MlpLstmPolicy', env, verbose=1, policy_kwargs=policy_kwargs, nminibatches=env_num, tensorboard_log=log_dir, n_steps=n_steps, noptepochs=4, train_num=train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tb_log_name : AutoLoopTrain_no_feat\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/stable_baselines/common/base_class.py:1169: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tqdm/_tqdm_notebook.py\u001b[0m in \u001b[0;36mstatus_printer\u001b[0;34m(_, total, desc, ncols)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m                 \u001b[0mpbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mIntProgress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# No total? Show info style bar with no progress tqdm status\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'IntProgress' is not defined",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-182f3c869751>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tb_log_name : {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtb_log_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_timesteps\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_log_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtb_log_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset_num_timesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;31m# ====================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-6d9c2ccd46b9>\u001b[0m in \u001b[0;36mlearn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps)\u001b[0m\n\u001b[1;32m    342\u001b[0m             \"\"\"\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mupdate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_updates\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    345\u001b[0m                 assert self.n_batch % self.nminibatches == 0, (\"The number of minibatches (`nminibatches`) \"\n\u001b[1;32m    346\u001b[0m                                                                \u001b[0;34m\"is not a factor of the total number of samples \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tqdm/__init__.py\u001b[0m in \u001b[0;36mtqdm_notebook\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;34m\"\"\"See tqdm._tqdm_notebook.tqdm_notebook for full documentation\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_tqdm_notebook\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtqdm_notebook\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_tqdm_notebook\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_tqdm_notebook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tqdm/_tqdm_notebook.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;31m# Replace with IPython progress bar display (with correct total)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         self.sp = self.status_printer(\n\u001b[0;32m--> 212\u001b[0;31m             self.fp, self.total, self.desc, self.ncols)\n\u001b[0m\u001b[1;32m    213\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdesc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m  \u001b[0;31m# trick to place description before the bar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/tqdm/_tqdm_notebook.py\u001b[0m in \u001b[0;36mstatus_printer\u001b[0;34m(_, total, desc, ncols)\u001b[0m\n\u001b[1;32m    109\u001b[0m             \u001b[0;31m# #187 #451 #558\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             raise ImportError(\n\u001b[0;32m--> 111\u001b[0;31m                 \u001b[0;34m\"IntProgress not found. Please update jupyter and ipywidgets.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m                 \u001b[0;34m\" See https://ipywidgets.readthedocs.io/en/stable\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 \"/user_install.html\")\n",
      "\u001b[0;31mImportError\u001b[0m: IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html"
     ]
    }
   ],
   "source": [
    "#for i in tqdm(range(int(resume_idx),  5)):\n",
    "\n",
    "i = 0\n",
    "# ====================================================\n",
    "# callback\n",
    "#\n",
    "# -------\n",
    "# eval callback\n",
    "#\n",
    "eval_callback = EvalCallback2(env_marker_test2, best_model_save_path='./logs/best_model',\n",
    "                            log_path='./logs/results', eval_freq=eval_freq, verbose=2, name_prefix='PPO2_{:09d}_'.format(i))\n",
    "# ------0\n",
    "# checkpoint callback\n",
    "#\n",
    "checkpoint_callback = CheckpointCallback2(save_freq=save_freq, save_path='./logs/', name_prefix='PPO2_{:09d}'.format(i), verbose=2)\n",
    "\n",
    "# -------\n",
    "# merge callback\n",
    "#\n",
    "#callback = CallbackList([checkpoint_callback, eval_callback, ProgressBarCallback(total_timesteps)])\n",
    "callback = CallbackList([checkpoint_callback, eval_callback, ])\n",
    "\n",
    "_model_name, _resume_idx, _train_num, _resume_FLAG = get_latest_model_param()\n",
    "if(_resume_FLAG):\n",
    "    # ====================================================\n",
    "    # Model setting\n",
    "    #\n",
    "    model = PPO2.load(\"./logs/{}_{:09d}_{:09d}_steps\".format(_model_name, int(_resume_idx), int(_train_num)))\n",
    "    model.set_env(env)\n",
    "    model.train_num = _train_num\n",
    "\n",
    "    # -------\n",
    "    # tensorboard\n",
    "    #\n",
    "    model.tensorboard_log = \"logs\"\n",
    "    model.num_timesteps = _train_num\n",
    "\n",
    "#tb_log_name = \"sampleE{}v\".format(i)\n",
    "tb_log_name = \"AutoLoopTrain_no_feat\"\n",
    "print(\"tb_log_name : {}\".format(tb_log_name))\n",
    "\n",
    "model.learn(total_timesteps=total_timesteps , log_interval=100, tb_log_name=tb_log_name, reset_num_timesteps=False, callback=callback)\n",
    "\n",
    "# ====================================================\n",
    "# Save model\n",
    "#\n",
    "model.save(\"./logs/PPO2_{:09d}_{:09d}_steps\".format(i, total_timesteps*(i+1)*env_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
