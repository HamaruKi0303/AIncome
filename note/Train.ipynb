{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/aincome\n"
     ]
    }
   ],
   "source": [
    "%cd /aincome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/aincome\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/stable_baselines/__init__.py:33: UserWarning: stable-baselines is in maintenance mode, please use [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) for an up-to-date version. You can find a [migration guide](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html) in SB3 documentation.\n",
      "  \"stable-baselines is in maintenance mode, please use [Stable-Baselines3 (SB3)](https://github.com/DLR-RM/stable-baselines3) for an up-to-date version. You can find a [migration guide](https://stable-baselines3.readthedocs.io/en/master/guide/migration.html) in SB3 documentation.\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "from tqdm import tqdm\n",
    "#from tqdm.notebook import tqdm\n",
    "#from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import warnings\n",
    "import typing\n",
    "from typing import Union, List, Dict, Any, Optional\n",
    "\n",
    "import gym\n",
    "import gym_anytrading\n",
    "from gym_anytrading.envs import TradingEnv, ForexEnv, StocksEnv, Actions, Positions\n",
    "from gym_anytrading.datasets import FOREX_EURUSD_1H_ASK, STOCKS_GOOGL\n",
    "\n",
    "from stable_baselines.bench import Monitor\n",
    "\n",
    "from stable_baselines.common.vec_env import VecEnv, sync_envs_normalization, DummyVecEnv\n",
    "from stable_baselines.common.vec_env import DummyVecEnv, SubprocVecEnv\n",
    "from stable_baselines.common.policies import MlpPolicy\n",
    "from stable_baselines.common import make_vec_env\n",
    "from stable_baselines import PPO2\n",
    "from stable_baselines import ACKTR\n",
    "from stable_baselines import A2C\n",
    "from stable_baselines.common.callbacks import CallbackList, CheckpointCallback, EvalCallback, EventCallback\n",
    "from stable_baselines.common.callbacks import BaseCallback\n",
    "from stable_baselines.common.evaluation import evaluate_policy\n",
    "\n",
    "import quantstats as qs\n",
    "import mplfinance as mpf\n",
    "\n",
    "import talib\n",
    "from yahoo_finance_api2 import share\n",
    "import yfinance\n",
    "from Historic_Crypto import Cryptocurrencies\n",
    "from Historic_Crypto import HistoricalData\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引数情報\n",
    "symbol      = 'XLM-USD'          # 通貨レート\n",
    "granularity = 300              # 何秒置きにデータを取得するか(60, 300, 900, 3600, 21600, 86400) が指定可能\n",
    "start_date  = '2020-01-01-00-00' # データ取得範囲：開始日\n",
    "end_date    = '2020-01-29-00-00' # データ取得範囲：終了日\n",
    "\n",
    "dataset_name = \"./data/datasets/v1.0/symbol-{}_granularity-{}_start-{}_end-{}.csv\".format(symbol, granularity, start_date, end_date)\n",
    "dataset_train_name = \"./data/datasets/v1.0/symbol-{}_granularity-{}_start-{}_end-{}_train.csv\".format(symbol, granularity, start_date, end_date)\n",
    "dataset_valid_name = \"./data/datasets/v1.0/symbol-{}_granularity-{}_start-{}_end-{}_valid.csv\".format(symbol, granularity, start_date, end_date)\n",
    "\n",
    "\n",
    "# ログフォルダの生成\n",
    "log_dir = './logs/'\n",
    "datasets_dir = '../datasets/'\n",
    "\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "os.makedirs(datasets_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "# train data idx\n",
    "idx1 = 100\n",
    "idx2 = 5000\n",
    "\n",
    "# test data idx\n",
    "idx3 = 6000\n",
    "\n",
    "window_size = 100\n",
    "\n",
    "trade_fee = 0\n",
    "\n",
    "env_num = 10\n",
    "\n",
    "total_timesteps=100000\n",
    "\n",
    "# tb_log_name = \"PPO2_feat19\"\n",
    "tb_log_name = \"PPO2_feat57_100_lstm128\"\n",
    "\n",
    "DATASET_GET_FLAG = False\n",
    "\n",
    "n_steps = 128\n",
    "\n",
    "save_freq = 100\n",
    "eval_freq = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_ti = pd.read_csv(dataset_train_name, index_col=0)\n",
    "df_test_ti = pd.read_csv(dataset_valid_name, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# データ読み込み部分\n",
    "#\n",
    "def my_process_data(env):\n",
    "    start = env.frame_bound[0] - env.window_size\n",
    "    end = env.frame_bound[1]\n",
    "    prices = env.df.loc[:, 'Low'].to_numpy()[start:end]\n",
    "\n",
    "    # -----------------------------\n",
    "    # 特徴量生成\n",
    "    #\n",
    "    df_features = env.df.copy()\n",
    "    # df_features.drop(\"\")\n",
    "    ohlc_features = df_features.loc[:, :].to_numpy()[start:end]\n",
    "\n",
    "    #print(ohlc_features.shape)\n",
    "    #print(np.diff(ohlc_features, axis=0).shape)\n",
    "    diff1 = np.insert(np.diff(ohlc_features, axis=0), 0, 0, axis=0)\n",
    "    diff2 = np.insert(np.diff(diff1, axis=0), 0, 0, axis=0)\n",
    "    #print(diff)\n",
    "    #signal_features = env.df.loc[:, ['Close', 'Open', 'High', 'Low', 'Vol']].to_numpy()[start:end]\n",
    "\n",
    "    #signal_features = np.column_stack((ohlc_features, diff1, diff2))\n",
    "    signal_features = np.column_stack((ohlc_features, ))\n",
    "\n",
    "    print(\">>> signal_features.shape\")\n",
    "    print( signal_features.shape)\n",
    "    #print( signal_features.head(5))\n",
    "\n",
    "    return prices, signal_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# 環境クラス\n",
    "#\n",
    "class MyBTCEnv(ForexEnv):\n",
    "    _process_data = my_process_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5637"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df_train_ti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> signal_features.shape\n",
      "(2417, 5)\n",
      ">>> signal_features.shape\n",
      "(5637, 5)\n",
      ">>> signal_features.shape\n",
      "(5637, 5)\n",
      ">>> signal_features.shape\n",
      "(5637, 5)\n",
      ">>> signal_features.shape\n",
      "(5637, 5)\n",
      ">>> signal_features.shape\n",
      "(5637, 5)\n",
      ">>> signal_features.shape\n",
      "(5637, 5)\n",
      ">>> signal_features.shape\n",
      "(5637, 5)\n",
      ">>> signal_features.shape\n",
      "(5637, 5)\n",
      ">>> signal_features.shape\n",
      "(5637, 5)\n",
      ">>> signal_features.shape\n",
      "(5637, 5)\n",
      ">>> signal_features.shape\n",
      "(2417, 5)\n"
     ]
    }
   ],
   "source": [
    "#####################################\n",
    "# 環境の生成\n",
    "#\n",
    "env_marker_train = lambda:  MyBTCEnv(df=df_train_ti, window_size=window_size, frame_bound=(window_size, len(df_train_ti)))\n",
    "env_marker_train.trade_fee = trade_fee\n",
    "\n",
    "env_marker_test = lambda:  MyBTCEnv(df=df_test_ti, window_size=window_size, frame_bound=(window_size, len(df_test_ti)))\n",
    "env_marker_test.trade_fee = trade_fee\n",
    "\n",
    "env_marker_test2 = env_marker_test()\n",
    "env_marker_test2.trade_fee = trade_fee\n",
    "\n",
    "env = DummyVecEnv([env_marker_train for _ in range(env_num)])\n",
    "#env = SubprocVecEnv([env_marker_train for i in range(env_num)])\n",
    "\n",
    "env_test = DummyVecEnv([env_marker_test for _ in range(1)])\n",
    "\n",
    "\n",
    "\n",
    "#env = Monitor(env, log_dir, allow_early_resets=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CheckpointCallback2(BaseCallback):\n",
    "    \"\"\"\n",
    "    Callback for saving a model every `save_freq` steps\n",
    "\n",
    "    :param save_freq: (int)\n",
    "    :param save_path: (str) Path to the folder where the model will be saved.\n",
    "    :param name_prefix: (str) Common prefix to the saved models\n",
    "    \"\"\"\n",
    "    def __init__(self, save_freq: int, save_path: str, name_prefix='rl_model', verbose=0):\n",
    "        super(CheckpointCallback2, self).__init__(verbose)\n",
    "        self.save_freq = save_freq\n",
    "        self.save_path = save_path\n",
    "        self.name_prefix = name_prefix\n",
    "\n",
    "    def _init_callback(self) -> None:\n",
    "        # Create folder if needed\n",
    "        if self.save_path is not None:\n",
    "            os.makedirs(self.save_path, exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        if self.n_calls % self.save_freq == 0:\n",
    "            path = os.path.join(self.save_path, '{}_{:09d}_steps'.format(self.name_prefix, self.num_timesteps))\n",
    "            self.model.save(path)\n",
    "            if self.verbose > 1:\n",
    "                print(\"Saving model checkpoint to {}\".format(path))\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvalCallback2(EventCallback):\n",
    "    \"\"\"\n",
    "    Callback for evaluating an agent.\n",
    "\n",
    "    :param eval_env: (Union[gym.Env, VecEnv]) The environment used for initialization\n",
    "    :param callback_on_new_best: (Optional[BaseCallback]) Callback to trigger\n",
    "        when there is a new best model according to the `mean_reward`\n",
    "    :param n_eval_episodes: (int) The number of episodes to test the agent\n",
    "    :param eval_freq: (int) Evaluate the agent every eval_freq call of the callback.\n",
    "    :param log_path: (str) Path to a folder where the evaluations (`evaluations.npz`)\n",
    "        will be saved. It will be updated at each evaluation.\n",
    "    :param best_model_save_path: (str) Path to a folder where the best model\n",
    "        according to performance on the eval env will be saved.\n",
    "    :param deterministic: (bool) Whether the evaluation should\n",
    "        use a stochastic or deterministic actions.\n",
    "    :param render: (bool) Whether to render or not the environment during evaluation\n",
    "    :param verbose: (int)\n",
    "    \"\"\"\n",
    "    def __init__(self, eval_env: Union[gym.Env, VecEnv],\n",
    "                 callback_on_new_best: Optional[BaseCallback] = None,\n",
    "                 n_eval_episodes: int = 5,\n",
    "                 eval_freq: int = 10000,\n",
    "                 log_path: str = None,\n",
    "                 best_model_save_path: str = None,\n",
    "                 deterministic: bool = True,\n",
    "                 render: bool = False,\n",
    "                 verbose: int = 1,\n",
    "                 name_prefix:str = None):\n",
    "      \n",
    "        super(EvalCallback2, self).__init__(callback_on_new_best, verbose=verbose)\n",
    "        self.n_eval_episodes = n_eval_episodes\n",
    "        self.eval_freq = eval_freq\n",
    "        self.best_mean_reward = -np.inf\n",
    "        self.last_mean_reward = -np.inf\n",
    "        self.deterministic = deterministic\n",
    "        self.render = render\n",
    "        self.name_prefix = name_prefix\n",
    "\n",
    "        # Convert to VecEnv for consistency\n",
    "        if not isinstance(eval_env, VecEnv):\n",
    "            eval_env = DummyVecEnv([lambda: eval_env])\n",
    "\n",
    "        assert eval_env.num_envs == 1, \"You must pass only one environment for evaluation\"\n",
    "\n",
    "        self.eval_env = eval_env\n",
    "        self.best_model_save_path = best_model_save_path\n",
    "        # Logs will be written in `evaluations.npz`\n",
    "        if log_path is not None:\n",
    "            log_path = os.path.join(log_path, 'evaluations')\n",
    "        self.log_path = log_path\n",
    "        self.evaluations_results = []\n",
    "        self.evaluations_timesteps = []\n",
    "        self.evaluations_length = []\n",
    "\n",
    "    def _init_callback(self):\n",
    "        # Does not work in some corner cases, where the wrapper is not the same\n",
    "        if not type(self.training_env) is type(self.eval_env):\n",
    "            warnings.warn(\"Training and eval env are not of the same type\"\n",
    "                          \"{} != {}\".format(self.training_env, self.eval_env))\n",
    "\n",
    "        # Create folders if needed\n",
    "        if self.best_model_save_path is not None:\n",
    "            os.makedirs(self.best_model_save_path, exist_ok=True)\n",
    "        if self.log_path is not None:\n",
    "            os.makedirs(os.path.dirname(self.log_path), exist_ok=True)\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "\n",
    "        if self.eval_freq > 0 and self.n_calls % self.eval_freq == 0:\n",
    "            # Sync training and eval env if there is VecNormalize\n",
    "            sync_envs_normalization(self.training_env, self.eval_env)\n",
    "\n",
    "            episode_rewards, episode_lengths = evaluate_policy(self.model, self.eval_env,\n",
    "                                                               n_eval_episodes=self.n_eval_episodes,\n",
    "                                                               render=self.render,\n",
    "                                                               deterministic=self.deterministic,\n",
    "                                                               return_episode_rewards=True)\n",
    "\n",
    "            print(\">>> log_path >>>\")\n",
    "            self.log_path2 = self.log_path + \"-{:09d}\".format(self.num_timesteps)\n",
    "            print(self.log_path)\n",
    "            print(self.log_path2)\n",
    "\n",
    "            if self.log_path2 is not None:\n",
    "                self.evaluations_timesteps.append(self.num_timesteps)\n",
    "                self.evaluations_results.append(episode_rewards)\n",
    "                self.evaluations_length.append(episode_lengths)\n",
    "                np.savez(self.log_path2, timesteps=self.evaluations_timesteps,\n",
    "                         results=self.evaluations_results, ep_lengths=self.evaluations_length)\n",
    "            \n",
    "            print(\">>> episode_rewards >>>\")\n",
    "            print(episode_rewards)\n",
    "\n",
    "\n",
    "            mean_reward, std_reward = np.mean(episode_rewards), np.std(episode_rewards)\n",
    "            mean_ep_length, std_ep_length = np.mean(episode_lengths), np.std(episode_lengths)\n",
    "            # Keep track of the last evaluation, useful for classes that derive from this callback\n",
    "            self.last_mean_reward = mean_reward\n",
    "\n",
    "            if self.verbose > 0:\n",
    "                print(\"Eval num_timesteps={}, \"\n",
    "                      \"episode_reward={:.2f} +/- {:.2f}\".format(self.num_timesteps, mean_reward, std_reward))\n",
    "                print(\"Episode length: {:.2f} +/- {:.2f}\".format(mean_ep_length, std_ep_length))\n",
    "\n",
    "            if mean_reward > self.best_mean_reward:\n",
    "                if self.verbose > 0:\n",
    "                    print(\"New best mean reward!\")\n",
    "                if self.best_model_save_path is not None:\n",
    "                    self.model.save(os.path.join(self.best_model_save_path, 'best_model'))\n",
    "                self.best_mean_reward = mean_reward\n",
    "                # Trigger callback if needed\n",
    "                if self.callback is not None:\n",
    "                    return self._on_event()\n",
    "\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------\n",
    "# first train\n",
    "#\n",
    "policy_kwargs = dict(net_arch=[64*2, 'lstm', dict(vf=[128, 128, 128], pi=[64*2, 64*2])])\n",
    "#model = PPO2('MlpLstmPolicy', env, verbose=0, policy_kwargs=policy_kwargs, nminibatches=env_num, tensorboard_log=log_dir, n_steps=n_steps)\n",
    "\n",
    "# ------\n",
    "# resume\n",
    "#\n",
    "#model = PPO2.load(\"/content/drive/MyDrive/AIncome/AutoTrade13/logs/rl_model_90000_steps\")\n",
    "#model.set_env(env)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_episode_reward_logger2(rew_acc, rewards, masks, writer, steps, n_steps=128, train_num=None):\n",
    "    \"\"\"\n",
    "    calculates the cumulated episode reward, and prints to tensorflow log the output\n",
    "\n",
    "    :param rew_acc: (np.array float) the total running reward\n",
    "    :param rewards: (np.array float) the rewards\n",
    "    :param masks: (np.array bool) the end of episodes\n",
    "    :param writer: (TensorFlow Session.writer) the writer to log to\n",
    "    :param steps: (int) the current timestep\n",
    "    :return: (np.array float) the updated total running reward\n",
    "    :return: (np.array float) the updated total running reward\n",
    "    \"\"\"\n",
    "\n",
    "    #print(\">>>>> step : {}\".format(steps))\n",
    "    #print(\"rewards: {}\".format(rewards))\n",
    "\n",
    "    with tf.variable_scope(\"environment_info\", reuse=True):\n",
    "        for env_idx in range(rewards.shape[0]):\n",
    "            dones_idx = np.sort(np.argwhere(masks[env_idx]))\n",
    "\n",
    "            \"\"\"\n",
    "            print(\"masks    : {}\".format(masks))\n",
    "            print(\"dones_idx: {}\".format(dones_idx))\n",
    "            print(\"dones_idx: {}\".format(len(dones_idx)))\n",
    "            \"\"\"\n",
    "\n",
    "            if len(dones_idx) == 0:\n",
    "                rew_acc[env_idx] += sum(rewards[env_idx])\n",
    "            else:\n",
    "                rew_acc[env_idx] += sum(rewards[env_idx, :dones_idx[0, 0]])\n",
    "                summary = tf.Summary(value=[tf.Summary.Value(tag=\"episode_reward\", simple_value=rew_acc[env_idx])])\n",
    "               \n",
    "                \n",
    "\n",
    "                \n",
    "                stepA = int((int(steps/(train_num)) -1)*train_num + (env_idx+1)*(train_num/rewards.shape[0]))\n",
    "\n",
    "                \"\"\"\n",
    "                print(\"---=== step+   : {}\".format(steps + dones_idx[0, 0]))\n",
    "                print(\"--->>> step    : {}\".format(steps))\n",
    "                print(\"------ env_idx : {}\".format(env_idx))\n",
    "                print(\"---*** stepA   : {}\".format(stepA))\n",
    "                \"\"\"\n",
    "\n",
    "                writer.add_summary(summary, stepA)\n",
    "\n",
    "                for k in range(1, len(dones_idx[:, 0])):\n",
    "                    rew_acc[env_idx] = sum(rewards[env_idx, dones_idx[k - 1, 0]:dones_idx[k, 0]])\n",
    "                    summary = tf.Summary(value=[tf.Summary.Value(tag=\"episode_reward\", simple_value=rew_acc[env_idx])])\n",
    "                    #writer.add_summary(summary, steps + dones_idx[k, 0])\n",
    "                    writer.add_summary(summary, steps + dones_idx[k, 0])\n",
    "                    print(\"---*** step : {}\".format(steps + dones_idx[k, 0]))\n",
    "                rew_acc[env_idx] = sum(rewards[env_idx, dones_idx[-1, 0]:])\n",
    "\n",
    "    return rew_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from stable_baselines import logger\n",
    "from stable_baselines.common import explained_variance, ActorCriticRLModel, tf_util, SetVerbosity, TensorboardWriter\n",
    "from stable_baselines.common.runners import AbstractEnvRunner\n",
    "from stable_baselines.common.policies import ActorCriticPolicy, RecurrentActorCriticPolicy\n",
    "from stable_baselines.common.schedules import get_schedule_fn\n",
    "#from stable_baselines.common.tf_util import total_episode_reward_logger\n",
    "from stable_baselines.common.math_util import safe_mean\n",
    "\n",
    "\n",
    "class PPO2(ActorCriticRLModel):\n",
    "    \"\"\"\n",
    "    Proximal Policy Optimization algorithm (GPU version).\n",
    "    Paper: https://arxiv.org/abs/1707.06347\n",
    "\n",
    "    :param policy: (ActorCriticPolicy or str) The policy model to use (MlpPolicy, CnnPolicy, CnnLstmPolicy, ...)\n",
    "    :param env: (Gym environment or str) The environment to learn from (if registered in Gym, can be str)\n",
    "    :param gamma: (float) Discount factor\n",
    "    :param n_steps: (int) The number of steps to run for each environment per update\n",
    "        (i.e. batch size is n_steps * n_env where n_env is number of environment copies running in parallel)\n",
    "    :param ent_coef: (float) Entropy coefficient for the loss calculation\n",
    "    :param learning_rate: (float or callable) The learning rate, it can be a function\n",
    "    :param vf_coef: (float) Value function coefficient for the loss calculation\n",
    "    :param max_grad_norm: (float) The maximum value for the gradient clipping\n",
    "    :param lam: (float) Factor for trade-off of bias vs variance for Generalized Advantage Estimator\n",
    "    :param nminibatches: (int) Number of training minibatches per update. For recurrent policies,\n",
    "        the number of environments run in parallel should be a multiple of nminibatches.\n",
    "    :param noptepochs: (int) Number of epoch when optimizing the surrogate\n",
    "    :param cliprange: (float or callable) Clipping parameter, it can be a function\n",
    "    :param cliprange_vf: (float or callable) Clipping parameter for the value function, it can be a function.\n",
    "        This is a parameter specific to the OpenAI implementation. If None is passed (default),\n",
    "        then `cliprange` (that is used for the policy) will be used.\n",
    "        IMPORTANT: this clipping depends on the reward scaling.\n",
    "        To deactivate value function clipping (and recover the original PPO implementation),\n",
    "        you have to pass a negative value (e.g. -1).\n",
    "    :param verbose: (int) the verbosity level: 0 none, 1 training information, 2 tensorflow debug\n",
    "    :param tensorboard_log: (str) the log location for tensorboard (if None, no logging)\n",
    "    :param _init_setup_model: (bool) Whether or not to build the network at the creation of the instance\n",
    "    :param policy_kwargs: (dict) additional arguments to be passed to the policy on creation\n",
    "    :param full_tensorboard_log: (bool) enable additional logging when using tensorboard\n",
    "        WARNING: this logging can take a lot of space quickly\n",
    "    :param seed: (int) Seed for the pseudo-random generators (python, numpy, tensorflow).\n",
    "        If None (default), use random seed. Note that if you want completely deterministic\n",
    "        results, you must set `n_cpu_tf_sess` to 1.\n",
    "    :param n_cpu_tf_sess: (int) The number of threads for TensorFlow operations\n",
    "        If None, the number of cpu of the current machine will be used.\n",
    "    \"\"\"\n",
    "    def __init__(self, policy, env, gamma=0.99, n_steps=128, ent_coef=0.01, learning_rate=2.5e-4, vf_coef=0.5,\n",
    "                 max_grad_norm=0.5, lam=0.95, nminibatches=4, noptepochs=4, cliprange=0.2, cliprange_vf=None,\n",
    "                 verbose=0, tensorboard_log=None, _init_setup_model=True, policy_kwargs=None,\n",
    "                 full_tensorboard_log=False, seed=None, n_cpu_tf_sess=None, train_num=None):\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.cliprange = cliprange\n",
    "        self.cliprange_vf = cliprange_vf\n",
    "        self.n_steps = n_steps\n",
    "        self.ent_coef = ent_coef\n",
    "        self.vf_coef = vf_coef\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.gamma = gamma\n",
    "        self.lam = lam\n",
    "        self.nminibatches = nminibatches\n",
    "        self.noptepochs = noptepochs\n",
    "        self.tensorboard_log = tensorboard_log\n",
    "        self.full_tensorboard_log = full_tensorboard_log\n",
    "\n",
    "        self.action_ph = None\n",
    "        self.advs_ph = None\n",
    "        self.rewards_ph = None\n",
    "        self.old_neglog_pac_ph = None\n",
    "        self.old_vpred_ph = None\n",
    "        self.learning_rate_ph = None\n",
    "        self.clip_range_ph = None\n",
    "        self.entropy = None\n",
    "        self.vf_loss = None\n",
    "        self.pg_loss = None\n",
    "        self.approxkl = None\n",
    "        self.clipfrac = None\n",
    "        self._train = None\n",
    "        self.loss_names = None\n",
    "        self.train_model = None\n",
    "        self.act_model = None\n",
    "        self.value = None\n",
    "        self.n_batch = None\n",
    "        self.summary = None\n",
    "        self.p_id    = 0\n",
    "        self.train_num = train_num\n",
    "\n",
    "        super().__init__(policy=policy, env=env, verbose=verbose, requires_vec_env=True,\n",
    "                         _init_setup_model=_init_setup_model, policy_kwargs=policy_kwargs,\n",
    "                         seed=seed, n_cpu_tf_sess=n_cpu_tf_sess)\n",
    "\n",
    "        if _init_setup_model:\n",
    "            self.setup_model()\n",
    "\n",
    "    def _make_runner(self):\n",
    "        return Runner(env=self.env, model=self, n_steps=self.n_steps,\n",
    "                      gamma=self.gamma, lam=self.lam)\n",
    "\n",
    "    def _get_pretrain_placeholders(self):\n",
    "        policy = self.act_model\n",
    "        if isinstance(self.action_space, gym.spaces.Discrete):\n",
    "            return policy.obs_ph, self.action_ph, policy.policy\n",
    "        return policy.obs_ph, self.action_ph, policy.deterministic_action\n",
    "\n",
    "    def setup_model(self):\n",
    "        with SetVerbosity(self.verbose):\n",
    "\n",
    "            assert issubclass(self.policy, ActorCriticPolicy), \"Error: the input policy for the PPO2 model must be \" \\\n",
    "                                                               \"an instance of common.policies.ActorCriticPolicy.\"\n",
    "\n",
    "            self.n_batch = self.n_envs * self.n_steps\n",
    "\n",
    "            self.graph = tf.Graph()\n",
    "            with self.graph.as_default():\n",
    "                self.set_random_seed(self.seed)\n",
    "                self.sess = tf_util.make_session(num_cpu=self.n_cpu_tf_sess, graph=self.graph)\n",
    "\n",
    "                n_batch_step = None\n",
    "                n_batch_train = None\n",
    "                if issubclass(self.policy, RecurrentActorCriticPolicy):\n",
    "                    assert self.n_envs % self.nminibatches == 0, \"For recurrent policies, \"\\\n",
    "                        \"the number of environments run in parallel should be a multiple of nminibatches.\"\n",
    "                    n_batch_step = self.n_envs\n",
    "                    n_batch_train = self.n_batch // self.nminibatches\n",
    "\n",
    "                act_model = self.policy(self.sess, self.observation_space, self.action_space, self.n_envs, 1,\n",
    "                                        n_batch_step, reuse=False, **self.policy_kwargs)\n",
    "                with tf.variable_scope(\"train_model\", reuse=True,\n",
    "                                       custom_getter=tf_util.outer_scope_getter(\"train_model\")):\n",
    "                    train_model = self.policy(self.sess, self.observation_space, self.action_space,\n",
    "                                              self.n_envs // self.nminibatches, self.n_steps, n_batch_train,\n",
    "                                              reuse=True, **self.policy_kwargs)\n",
    "\n",
    "                with tf.variable_scope(\"loss\", reuse=False):\n",
    "                    self.action_ph = train_model.pdtype.sample_placeholder([None], name=\"action_ph\")\n",
    "                    self.advs_ph = tf.placeholder(tf.float32, [None], name=\"advs_ph\")\n",
    "                    self.rewards_ph = tf.placeholder(tf.float32, [None], name=\"rewards_ph\")\n",
    "                    self.old_neglog_pac_ph = tf.placeholder(tf.float32, [None], name=\"old_neglog_pac_ph\")\n",
    "                    self.old_vpred_ph = tf.placeholder(tf.float32, [None], name=\"old_vpred_ph\")\n",
    "                    self.learning_rate_ph = tf.placeholder(tf.float32, [], name=\"learning_rate_ph\")\n",
    "                    self.clip_range_ph = tf.placeholder(tf.float32, [], name=\"clip_range_ph\")\n",
    "\n",
    "                    neglogpac = train_model.proba_distribution.neglogp(self.action_ph)\n",
    "                    self.entropy = tf.reduce_mean(train_model.proba_distribution.entropy())\n",
    "\n",
    "                    vpred = train_model.value_flat\n",
    "\n",
    "                    # Value function clipping: not present in the original PPO\n",
    "                    if self.cliprange_vf is None:\n",
    "                        # Default behavior (legacy from OpenAI baselines):\n",
    "                        # use the same clipping as for the policy\n",
    "                        self.clip_range_vf_ph = self.clip_range_ph\n",
    "                        self.cliprange_vf = self.cliprange\n",
    "                    elif isinstance(self.cliprange_vf, (float, int)) and self.cliprange_vf < 0:\n",
    "                        # Original PPO implementation: no value function clipping\n",
    "                        self.clip_range_vf_ph = None\n",
    "                    else:\n",
    "                        # Last possible behavior: clipping range\n",
    "                        # specific to the value function\n",
    "                        self.clip_range_vf_ph = tf.placeholder(tf.float32, [], name=\"clip_range_vf_ph\")\n",
    "\n",
    "                    if self.clip_range_vf_ph is None:\n",
    "                        # No clipping\n",
    "                        vpred_clipped = train_model.value_flat\n",
    "                    else:\n",
    "                        # Clip the different between old and new value\n",
    "                        # NOTE: this depends on the reward scaling\n",
    "                        vpred_clipped = self.old_vpred_ph + \\\n",
    "                            tf.clip_by_value(train_model.value_flat - self.old_vpred_ph,\n",
    "                                             - self.clip_range_vf_ph, self.clip_range_vf_ph)\n",
    "\n",
    "                    vf_losses1 = tf.square(vpred - self.rewards_ph)\n",
    "                    vf_losses2 = tf.square(vpred_clipped - self.rewards_ph)\n",
    "                    self.vf_loss = .5 * tf.reduce_mean(tf.maximum(vf_losses1, vf_losses2))\n",
    "\n",
    "                    ratio = tf.exp(self.old_neglog_pac_ph - neglogpac)\n",
    "                    pg_losses = -self.advs_ph * ratio\n",
    "                    pg_losses2 = -self.advs_ph * tf.clip_by_value(ratio, 1.0 - self.clip_range_ph, 1.0 +\n",
    "                                                                  self.clip_range_ph)\n",
    "                    self.pg_loss = tf.reduce_mean(tf.maximum(pg_losses, pg_losses2))\n",
    "                    self.approxkl = .5 * tf.reduce_mean(tf.square(neglogpac - self.old_neglog_pac_ph))\n",
    "                    self.clipfrac = tf.reduce_mean(tf.cast(tf.greater(tf.abs(ratio - 1.0),\n",
    "                                                                      self.clip_range_ph), tf.float32))\n",
    "                    loss = self.pg_loss - self.entropy * self.ent_coef + self.vf_loss * self.vf_coef\n",
    "\n",
    "                    tf.summary.scalar('entropy_loss', self.entropy)\n",
    "                    \"\"\"\n",
    "                    print(\"------------------------------\")\n",
    "                    print(\" self.entropy \")\n",
    "                    print(self.entropy)\n",
    "                    \"\"\"\n",
    "\n",
    "                    tf.summary.scalar('policy_gradient_loss', self.pg_loss)\n",
    "                    tf.summary.scalar('value_function_loss', self.vf_loss)\n",
    "                    tf.summary.scalar('approximate_kullback-leibler', self.approxkl)\n",
    "                    tf.summary.scalar('clip_factor', self.clipfrac)\n",
    "                    tf.summary.scalar('loss', loss)\n",
    "\n",
    "                    with tf.variable_scope('model'):\n",
    "                        self.params = tf.trainable_variables()\n",
    "                        if self.full_tensorboard_log:\n",
    "                            for var in self.params:\n",
    "                                tf.summary.histogram(var.name, var)\n",
    "                    grads = tf.gradients(loss, self.params)\n",
    "                    if self.max_grad_norm is not None:\n",
    "                        grads, _grad_norm = tf.clip_by_global_norm(grads, self.max_grad_norm)\n",
    "                    grads = list(zip(grads, self.params))\n",
    "                trainer = tf.train.AdamOptimizer(learning_rate=self.learning_rate_ph, epsilon=1e-5)\n",
    "                self._train = trainer.apply_gradients(grads)\n",
    "\n",
    "                self.loss_names = ['policy_loss', 'value_loss', 'policy_entropy', 'approxkl', 'clipfrac']\n",
    "\n",
    "                with tf.variable_scope(\"input_info\", reuse=False):\n",
    "                    tf.summary.scalar('discounted_rewards', tf.reduce_mean(self.rewards_ph))\n",
    "                    tf.summary.scalar('learning_rate', tf.reduce_mean(self.learning_rate_ph))\n",
    "                    tf.summary.scalar('advantage', tf.reduce_mean(self.advs_ph))\n",
    "                    tf.summary.scalar('clip_range', tf.reduce_mean(self.clip_range_ph))\n",
    "                    if self.clip_range_vf_ph is not None:\n",
    "                        tf.summary.scalar('clip_range_vf', tf.reduce_mean(self.clip_range_vf_ph))\n",
    "\n",
    "                    tf.summary.scalar('old_neglog_action_probability', tf.reduce_mean(self.old_neglog_pac_ph))\n",
    "                    tf.summary.scalar('old_value_pred', tf.reduce_mean(self.old_vpred_ph))\n",
    "\n",
    "                    if self.full_tensorboard_log:\n",
    "                        tf.summary.histogram('discounted_rewards', self.rewards_ph)\n",
    "                        tf.summary.histogram('learning_rate', self.learning_rate_ph)\n",
    "                        tf.summary.histogram('advantage', self.advs_ph)\n",
    "                        tf.summary.histogram('clip_range', self.clip_range_ph)\n",
    "                        tf.summary.histogram('old_neglog_action_probability', self.old_neglog_pac_ph)\n",
    "                        tf.summary.histogram('old_value_pred', self.old_vpred_ph)\n",
    "                        if tf_util.is_image(self.observation_space):\n",
    "                            tf.summary.image('observation', train_model.obs_ph)\n",
    "                        else:\n",
    "                            tf.summary.histogram('observation', train_model.obs_ph)\n",
    "\n",
    "                self.train_model = train_model\n",
    "                self.act_model = act_model\n",
    "                self.step = act_model.step\n",
    "                self.proba_step = act_model.proba_step\n",
    "                self.value = act_model.value\n",
    "                self.initial_state = act_model.initial_state\n",
    "                tf.global_variables_initializer().run(session=self.sess)  # pylint: disable=E1101\n",
    "\n",
    "                self.summary = tf.summary.merge_all()\n",
    "\n",
    "\n",
    "    def _train_step(self, learning_rate, cliprange, obs, returns, masks, actions, values, neglogpacs, update,\n",
    "                    writer, states=None, cliprange_vf=None):\n",
    "        \"\"\"\n",
    "        Training of PPO2 Algorithm\n",
    "\n",
    "        :param learning_rate: (float) learning rate\n",
    "        :param cliprange: (float) Clipping factor\n",
    "        :param obs: (np.ndarray) The current observation of the environment\n",
    "        :param returns: (np.ndarray) the rewards\n",
    "        :param masks: (np.ndarray) The last masks for done episodes (used in recurent policies)\n",
    "        :param actions: (np.ndarray) the actions\n",
    "        :param values: (np.ndarray) the values\n",
    "        :param neglogpacs: (np.ndarray) Negative Log-likelihood probability of Actions\n",
    "        :param update: (int) the current step iteration\n",
    "        :param writer: (TensorFlow Summary.writer) the writer for tensorboard\n",
    "        :param states: (np.ndarray) For recurrent policies, the internal state of the recurrent model\n",
    "        :return: policy gradient loss, value function loss, policy entropy,\n",
    "                approximation of kl divergence, updated clipping range, training update operation\n",
    "        :param cliprange_vf: (float) Clipping factor for the value function\n",
    "        \"\"\"\n",
    "        advs = returns - values\n",
    "        advs = (advs - advs.mean()) / (advs.std() + 1e-8)\n",
    "        td_map = {self.train_model.obs_ph: obs, self.action_ph: actions,\n",
    "                  self.advs_ph: advs, self.rewards_ph: returns,\n",
    "                  self.learning_rate_ph: learning_rate, self.clip_range_ph: cliprange,\n",
    "                  self.old_neglog_pac_ph: neglogpacs, self.old_vpred_ph: values}\n",
    "        if states is not None:\n",
    "            td_map[self.train_model.states_ph] = states\n",
    "            td_map[self.train_model.dones_ph] = masks\n",
    "\n",
    "        if cliprange_vf is not None and cliprange_vf >= 0:\n",
    "            td_map[self.clip_range_vf_ph] = cliprange_vf\n",
    "\n",
    "        if states is None:\n",
    "            update_fac = max(self.n_batch // self.nminibatches // self.noptepochs, 1)\n",
    "        else:\n",
    "            update_fac = max(self.n_batch // self.nminibatches // self.noptepochs // self.n_steps, 1)\n",
    "\n",
    "        if writer is not None:\n",
    "            # run loss backprop with summary, but once every 10 runs save the metadata (memory, compute time, ...)\n",
    "            if self.full_tensorboard_log and (1 + update) % 10 == 0:\n",
    "                run_options = tf.RunOptions(trace_level=tf.RunOptions.FULL_TRACE)\n",
    "                run_metadata = tf.RunMetadata()\n",
    "                summary, policy_loss, value_loss, policy_entropy, approxkl, clipfrac, _ = self.sess.run(\n",
    "                    [self.summary, self.pg_loss, self.vf_loss, self.entropy, self.approxkl, self.clipfrac, self._train],\n",
    "                    td_map, options=run_options, run_metadata=run_metadata)\n",
    "                #print(\"update---->{}\".format(update))\n",
    "                writer.add_run_metadata(run_metadata, 'step%d' % (update * update_fac))\n",
    "            else:\n",
    "                summary, policy_loss, value_loss, policy_entropy, approxkl, clipfrac, _ = self.sess.run(\n",
    "                    [self.summary, self.pg_loss, self.vf_loss, self.entropy, self.approxkl, self.clipfrac, self._train],\n",
    "                    td_map)\n",
    "            writer.add_summary(summary, (update * update_fac))\n",
    "            \"\"\"\n",
    "            print(\">>>>>>>>>>>>>>>>>>>>>>>\")\n",
    "            print(\"update             : {}\".format(update))\n",
    "            print(\"update_fac         : {}\".format(update_fac))\n",
    "            print(\"update*update_fac  : {}\".format(update * update_fac))\n",
    "            print(\"self.entropy:      : {}\".format(self.entropy))\n",
    "            \"\"\"\n",
    "        else:\n",
    "            policy_loss, value_loss, policy_entropy, approxkl, clipfrac, _ = self.sess.run(\n",
    "                [self.pg_loss, self.vf_loss, self.entropy, self.approxkl, self.clipfrac, self._train], td_map)\n",
    "\n",
    "        return policy_loss, value_loss, policy_entropy, approxkl, clipfrac\n",
    "\n",
    "    def learn(self, total_timesteps, callback=None, log_interval=1, tb_log_name=\"PPO2\",\n",
    "              reset_num_timesteps=True):\n",
    "        # Transform to callable if needed\n",
    "        self.learning_rate = get_schedule_fn(self.learning_rate)\n",
    "        self.cliprange = get_schedule_fn(self.cliprange)\n",
    "        cliprange_vf = get_schedule_fn(self.cliprange_vf)\n",
    "\n",
    "        new_tb_log = self._init_num_timesteps(reset_num_timesteps)\n",
    "        callback = self._init_callback(callback)\n",
    "\n",
    "        with SetVerbosity(self.verbose), TensorboardWriter(self.graph, self.tensorboard_log, tb_log_name, new_tb_log) \\\n",
    "                as writer:\n",
    "            self._setup_learn()\n",
    "\n",
    "            t_first_start = time.time()\n",
    "            #n_updates = total_timesteps // self.n_batch\n",
    "            n_updates = total_timesteps\n",
    "\n",
    "            callback.on_training_start(locals(), globals())\n",
    "\n",
    "            \"\"\"\n",
    "            print(\"---------------------------------\")\n",
    "            print(\"n_updates + 1 : {}\".format(n_updates + 1))\n",
    "            \"\"\"\n",
    "\n",
    "            for update in tqdm(range(1, n_updates + 1)):\n",
    "                assert self.n_batch % self.nminibatches == 0, (\"The number of minibatches (`nminibatches`) \"\n",
    "                                                               \"is not a factor of the total number of samples \"\n",
    "                                                               \"collected per rollout (`n_batch`), \"\n",
    "                                                               \"some samples won't be used.\"\n",
    "                                                               )\n",
    "                batch_size = self.n_batch // self.nminibatches\n",
    "                t_start = time.time()\n",
    "                frac = 1.0 - (update - 1.0) / n_updates\n",
    "                lr_now = self.learning_rate(frac)\n",
    "                cliprange_now = self.cliprange(frac)\n",
    "                cliprange_vf_now = cliprange_vf(frac)\n",
    "\n",
    "                callback.on_rollout_start()\n",
    "                # true_reward is the reward without discount\n",
    "                rollout = self.runner.run(callback)\n",
    "                # Unpack\n",
    "                obs, returns, masks, actions, values, neglogpacs, states, ep_infos, true_reward = rollout\n",
    "\n",
    "                callback.on_rollout_end()\n",
    "\n",
    "                # Early stopping due to the callback\n",
    "                if not self.runner.continue_training:\n",
    "                    break\n",
    "\n",
    "                self.ep_info_buf.extend(ep_infos)\n",
    "                mb_loss_vals = []\n",
    "                if states is None:  # nonrecurrent version\n",
    "                    update_fac = max(self.n_batch // self.nminibatches // self.noptepochs, 1)\n",
    "                    inds = np.arange(self.n_batch)\n",
    "                    for epoch_num in range(self.noptepochs):\n",
    "                        np.random.shuffle(inds)\n",
    "                        for start in range(0, self.n_batch, batch_size):\n",
    "                            timestep = self.num_timesteps // update_fac + ((epoch_num *\n",
    "                                                                            self.n_batch + start) // batch_size)\n",
    "                            end = start + batch_size\n",
    "                            mbinds = inds[start:end]\n",
    "                            slices = (arr[mbinds] for arr in (obs, returns, masks, actions, values, neglogpacs))\n",
    "                            mb_loss_vals.append(self._train_step(lr_now, cliprange_now, *slices, writer=writer,\n",
    "                                                                 update=timestep, cliprange_vf=cliprange_vf_now))\n",
    "                else:  # recurrent version\n",
    "                    update_fac = max(self.n_batch // self.nminibatches // self.noptepochs // self.n_steps, 1)\n",
    "                    assert self.n_envs % self.nminibatches == 0\n",
    "                    env_indices = np.arange(self.n_envs)\n",
    "                    flat_indices = np.arange(self.n_envs * self.n_steps).reshape(self.n_envs, self.n_steps)\n",
    "                    envs_per_batch = batch_size // self.n_steps\n",
    "                    for epoch_num in range(1):\n",
    "                        np.random.shuffle(env_indices)\n",
    "                        for start in range(0, self.n_envs, envs_per_batch):\n",
    "                            timestep = self.num_timesteps // update_fac + ((epoch_num *\n",
    "                                                                            self.n_envs + start) // envs_per_batch)\n",
    "                            \n",
    "\n",
    "                            \"\"\"\n",
    "                            print(\">>>> timestep          :{}\".format(timestep))\n",
    "                            print(\">>>> self.num_timesteps:{}\".format(self.num_timesteps))\n",
    "                            print(\">>>> update_fac        :{}\".format(update_fac))\n",
    "                            print(\">>>> epoch_num         :{}\".format(epoch_num))\n",
    "                            print(\">>>> self.n_envs       :{}\".format(self.n_envs))\n",
    "                            print(\">>>> start             :{}\".format(start))\n",
    "                            print(\">>>> envs_per_batch    :{}\".format(envs_per_batch))\n",
    "                            print(\">>>> self.noptepochs   :{}\".format(self.noptepochs))\n",
    "                            print(\">>>>>>>> self.num_timesteps // update_fac + ((epoch_num * self.n_envs + start) // envs_per_batch)\")\n",
    "                            \"\"\"\n",
    "\n",
    "                            end = start + envs_per_batch\n",
    "                            mb_env_inds = env_indices[start:end]\n",
    "                            mb_flat_inds = flat_indices[mb_env_inds].ravel()\n",
    "                            slices = (arr[mb_flat_inds] for arr in (obs, returns, masks, actions, values, neglogpacs))\n",
    "                            mb_states = states[mb_env_inds]\n",
    "                            mb_loss_vals.append(self._train_step(lr_now, cliprange_now, *slices, update=timestep,\n",
    "                                                                 writer=writer, states=mb_states,\n",
    "                                                                 cliprange_vf=cliprange_vf_now))\n",
    "\n",
    "                loss_vals = np.mean(mb_loss_vals, axis=0)\n",
    "                t_now = time.time()\n",
    "                fps = int(self.n_batch / (t_now - t_start))\n",
    "\n",
    "                if writer is not None:\n",
    "                    \n",
    "                    #print(\">>>>>>> writer >>>>>>> \")\n",
    "                    \n",
    "\n",
    "                    total_episode_reward_logger2(self.episode_reward,\n",
    "                                                true_reward.reshape((self.n_envs, self.n_steps)),\n",
    "                                                masks.reshape((self.n_envs, self.n_steps)),\n",
    "                                                writer, self.num_timesteps, n_steps=self.n_steps, train_num=self.train_num)\n",
    "\n",
    "                if self.verbose >= 1 and (update % log_interval == 0 or update == 1):\n",
    "                    explained_var = explained_variance(values, returns)\n",
    "                    logger.logkv(\"serial_timesteps\", update * self.n_steps)\n",
    "                    logger.logkv(\"n_updates\", update)\n",
    "                    logger.logkv(\"total_timesteps\", self.num_timesteps)\n",
    "                    logger.logkv(\"fps\", fps)\n",
    "                    logger.logkv(\"explained_variance\", float(explained_var))\n",
    "                    if len(self.ep_info_buf) > 0 and len(self.ep_info_buf[0]) > 0:\n",
    "                        logger.logkv('ep_reward_mean', safe_mean([ep_info['r'] for ep_info in self.ep_info_buf]))\n",
    "                        logger.logkv('ep_len_mean', safe_mean([ep_info['l'] for ep_info in self.ep_info_buf]))\n",
    "                    logger.logkv('time_elapsed', t_start - t_first_start)\n",
    "                    for (loss_val, loss_name) in zip(loss_vals, self.loss_names):\n",
    "                        logger.logkv(loss_name, loss_val)\n",
    "                    logger.dumpkvs()\n",
    "\n",
    "            callback.on_training_end()\n",
    "            return self\n",
    "\n",
    "\n",
    "    def save(self, save_path, cloudpickle=False):\n",
    "        data = {\n",
    "            \"gamma\": self.gamma,\n",
    "            \"n_steps\": self.n_steps,\n",
    "            \"vf_coef\": self.vf_coef,\n",
    "            \"ent_coef\": self.ent_coef,\n",
    "            \"max_grad_norm\": self.max_grad_norm,\n",
    "            \"learning_rate\": self.learning_rate,\n",
    "            \"lam\": self.lam,\n",
    "            \"nminibatches\": self.nminibatches,\n",
    "            \"noptepochs\": self.noptepochs,\n",
    "            \"cliprange\": self.cliprange,\n",
    "            \"cliprange_vf\": self.cliprange_vf,\n",
    "            \"verbose\": self.verbose,\n",
    "            \"policy\": self.policy,\n",
    "            \"observation_space\": self.observation_space,\n",
    "            \"action_space\": self.action_space,\n",
    "            \"n_envs\": self.n_envs,\n",
    "            \"n_cpu_tf_sess\": self.n_cpu_tf_sess,\n",
    "            \"seed\": self.seed,\n",
    "            \"_vectorize_action\": self._vectorize_action,\n",
    "            \"policy_kwargs\": self.policy_kwargs\n",
    "        }\n",
    "\n",
    "        params_to_save = self.get_parameters()\n",
    "\n",
    "        self._save_to_file(save_path, data=data, params=params_to_save, cloudpickle=cloudpickle)\n",
    "\n",
    "\n",
    "\n",
    "class Runner(AbstractEnvRunner):\n",
    "    def __init__(self, *, env, model, n_steps, gamma, lam):\n",
    "        \"\"\"\n",
    "        A runner to learn the policy of an environment for a model\n",
    "\n",
    "        :param env: (Gym environment) The environment to learn from\n",
    "        :param model: (Model) The model to learn\n",
    "        :param n_steps: (int) The number of steps to run for each environment\n",
    "        :param gamma: (float) Discount factor\n",
    "        :param lam: (float) Factor for trade-off of bias vs variance for Generalized Advantage Estimator\n",
    "        \"\"\"\n",
    "        super().__init__(env=env, model=model, n_steps=n_steps)\n",
    "        self.lam = lam\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def _run(self):\n",
    "        \"\"\"\n",
    "        Run a learning step of the model\n",
    "\n",
    "        :return:\n",
    "            - observations: (np.ndarray) the observations\n",
    "            - rewards: (np.ndarray) the rewards\n",
    "            - masks: (numpy bool) whether an episode is over or not\n",
    "            - actions: (np.ndarray) the actions\n",
    "            - values: (np.ndarray) the value function output\n",
    "            - negative log probabilities: (np.ndarray)\n",
    "            - states: (np.ndarray) the internal states of the recurrent policies\n",
    "            - infos: (dict) the extra information of the model\n",
    "        \"\"\"\n",
    "        # mb stands for minibatch\n",
    "        mb_obs, mb_rewards, mb_actions, mb_values, mb_dones, mb_neglogpacs = [], [], [], [], [], []\n",
    "        mb_states = self.states\n",
    "        ep_infos = []\n",
    "\n",
    "        #print(\"-----------------------------\")\n",
    "        #print(\"self.model.num_timesteps : {}\".format(self.model.num_timesteps))\n",
    "        self.model.num_timesteps += self.n_envs       \n",
    "        ##print(\"self.model.num_timesteps : {}\".format(self.model.num_timesteps))\n",
    "        #print(\"self.n_envs : {}\".format(self.n_envs))\n",
    "\n",
    "        #self.model.num_timesteps += 1\n",
    "        #self.model.num_timesteps += 4\n",
    "\n",
    "        for _ in range(self.n_steps):\n",
    "            actions, values, self.states, neglogpacs = self.model.step(self.obs, self.states, self.dones)  # pytype: disable=attribute-error\n",
    "            mb_obs.append(self.obs.copy())\n",
    "            mb_actions.append(actions)\n",
    "            mb_values.append(values)\n",
    "            mb_neglogpacs.append(neglogpacs)\n",
    "            mb_dones.append(self.dones)\n",
    "            clipped_actions = actions\n",
    "            # Clip the actions to avoid out of bound error\n",
    "            if isinstance(self.env.action_space, gym.spaces.Box):\n",
    "                clipped_actions = np.clip(actions, self.env.action_space.low, self.env.action_space.high)\n",
    "            self.obs[:], rewards, self.dones, infos = self.env.step(clipped_actions)\n",
    "\n",
    "            #print(\"**** num_timesteps before : {}\".format(self.model.num_timesteps))\n",
    "            #self.model.num_timesteps += self.n_envs\n",
    "            #print(\"**** num_timesteps after  : {}\".format(self.model.num_timesteps))\n",
    "\n",
    "            \"\"\"\n",
    "            if self.callback is not None:\n",
    "                # Abort training early\n",
    "                self.callback.update_locals(locals())\n",
    "                if self.callback.on_step() is False:\n",
    "                    self.continue_training = False\n",
    "                    # Return dummy values\n",
    "                    return [None] * 9\n",
    "            \"\"\"\n",
    "            \n",
    "            for info in infos:\n",
    "                maybe_ep_info = info.get('episode')\n",
    "                if maybe_ep_info is not None:\n",
    "                    ep_infos.append(maybe_ep_info)\n",
    "            mb_rewards.append(rewards)\n",
    "        # batch of steps to batch of rollouts\n",
    "\n",
    "        if self.callback is not None:\n",
    "            # Abort training early\n",
    "            self.callback.update_locals(locals())\n",
    "            if self.callback.on_step() is False:\n",
    "                self.continue_training = False\n",
    "                # Return dummy values\n",
    "                return [None] * 9\n",
    "\n",
    "\n",
    "        mb_obs = np.asarray(mb_obs, dtype=self.obs.dtype)\n",
    "        mb_rewards = np.asarray(mb_rewards, dtype=np.float32)\n",
    "        mb_actions = np.asarray(mb_actions)\n",
    "        mb_values = np.asarray(mb_values, dtype=np.float32)\n",
    "        mb_neglogpacs = np.asarray(mb_neglogpacs, dtype=np.float32)\n",
    "        mb_dones = np.asarray(mb_dones, dtype=np.bool)\n",
    "        last_values = self.model.value(self.obs, self.states, self.dones)  # pytype: disable=attribute-error\n",
    "        # discount/bootstrap off value fn\n",
    "        mb_advs = np.zeros_like(mb_rewards)\n",
    "        true_reward = np.copy(mb_rewards)\n",
    "        last_gae_lam = 0\n",
    "        for step in reversed(range(self.n_steps)):\n",
    "            if step == self.n_steps - 1:\n",
    "                nextnonterminal = 1.0 - self.dones\n",
    "                nextvalues = last_values\n",
    "            else:\n",
    "                nextnonterminal = 1.0 - mb_dones[step + 1]\n",
    "                nextvalues = mb_values[step + 1]\n",
    "            delta = mb_rewards[step] + self.gamma * nextvalues * nextnonterminal - mb_values[step]\n",
    "            mb_advs[step] = last_gae_lam = delta + self.gamma * self.lam * nextnonterminal * last_gae_lam\n",
    "        mb_returns = mb_advs + mb_values\n",
    "\n",
    "        mb_obs, mb_returns, mb_dones, mb_actions, mb_values, mb_neglogpacs, true_reward = \\\n",
    "            map(swap_and_flatten, (mb_obs, mb_returns, mb_dones, mb_actions, mb_values, mb_neglogpacs, true_reward))\n",
    "\n",
    "        return mb_obs, mb_returns, mb_dones, mb_actions, mb_values, mb_neglogpacs, mb_states, ep_infos, true_reward\n",
    "\n",
    "\n",
    "# obs, returns, masks, actions, values, neglogpacs, states = runner.run()\n",
    "def swap_and_flatten(arr):\n",
    "    \"\"\"\n",
    "    swap and then flatten axes 0 and 1\n",
    "\n",
    "    :param arr: (np.ndarray)\n",
    "    :return: (np.ndarray)\n",
    "    \"\"\"\n",
    "    shape = arr.shape\n",
    "    return arr.swapaxes(0, 1).reshape(shape[0] * shape[1], *shape[2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "resume_FLAG = False\n",
    "resume_idx    = 0\n",
    "train_num     = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_num : 1\n"
     ]
    }
   ],
   "source": [
    "mean_reward_list = []\n",
    "\n",
    "print(\"train_num : {}\".format(train_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "# ---------\n",
    "# load save model\n",
    "model_list = sorted(glob.glob(\"logs/*.zip\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_model_param():\n",
    "    # ---------\n",
    "    # load save model\n",
    "    model_list = sorted(glob.glob(\"logs/*.zip\"))\n",
    "    resume_FLAG = False\n",
    "\n",
    "    if(len(model_list)>0):\n",
    "        model_latest = model_list[-1].split(\"/\")[-1].split(\".zip\")[0].split(\"_step\")[0]\n",
    "        resume_FLAG = True\n",
    "        return model_latest.split(\"_\")[0], int(model_latest.split(\"_\")[1]), int(model_latest.split(\"_\")[2]), resume_FLAG\n",
    "\n",
    "    return None, None, None, resume_FLAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:191: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/stable_baselines/common/tf_util.py:200: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/stable_baselines/common/policies.py:116: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/stable_baselines/common/input.py:25: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/stable_baselines/common/policies.py:442: flatten (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.flatten instead.\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f0970159ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f0970159ac8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f0970159ac8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f0970159ac8>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/stable_baselines/common/tf_layers.py:123: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f0970159860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f0970159860>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING: Entity <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f0970159860>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method Flatten.call of <tensorflow.python.layers.core.Flatten object at 0x7f0970159860>>: AttributeError: module 'gast' has no attribute 'Index'\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "# ====================================================\n",
    "# Create Env & Model\n",
    "#\n",
    "# env = gym.make('LunarLander-v2')\n",
    "#model = DQN('MlpPolicy', env, learning_rate=1e-3, prioritized_replay=True, verbose=0, tensorboard_log=log_dir, full_tensorboard_log=True)\n",
    "model = PPO2('MlpLstmPolicy', env, verbose=1, policy_kwargs=policy_kwargs, nminibatches=env_num, tensorboard_log=log_dir, n_steps=n_steps, noptepochs=4, train_num=train_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tb_log_name : AutoLoopTrain_no_feat\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/stable_baselines/common/base_class.py:1169: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| approxkl           | 0.00090187433  |\n",
      "| clipfrac           | 0.0            |\n",
      "| explained_variance | -0.0106        |\n",
      "| fps                | 164            |\n",
      "| n_updates          | 1              |\n",
      "| policy_entropy     | 0.6856524      |\n",
      "| policy_loss        | -0.00097085314 |\n",
      "| serial_timesteps   | 128            |\n",
      "| time_elapsed       | 0.0021         |\n",
      "| total_timesteps    | 10             |\n",
      "| value_loss         | 0.5466339      |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 99/100000 [02:14<36:10:40,  1.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000001000_steps\n",
      "--------------------------------------\n",
      "| approxkl           | 0.00246579    |\n",
      "| clipfrac           | 0.0125        |\n",
      "| explained_variance | -0.00116      |\n",
      "| fps                | 855           |\n",
      "| n_updates          | 100           |\n",
      "| policy_entropy     | 0.57174474    |\n",
      "| policy_loss        | -0.0034709251 |\n",
      "| serial_timesteps   | 12800         |\n",
      "| time_elapsed       | 135           |\n",
      "| total_timesteps    | 1000          |\n",
      "| value_loss         | 5.611031      |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 199/100000 [04:26<37:37:50,  1.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000002000_steps\n",
      "------------------------------------\n",
      "| approxkl           | 0.03275443  |\n",
      "| clipfrac           | 0.24296875  |\n",
      "| explained_variance | 0.0279      |\n",
      "| fps                | 961         |\n",
      "| n_updates          | 200         |\n",
      "| policy_entropy     | 0.32040578  |\n",
      "| policy_loss        | 0.024947241 |\n",
      "| serial_timesteps   | 25600       |\n",
      "| time_elapsed       | 266         |\n",
      "| total_timesteps    | 2000        |\n",
      "| value_loss         | 2.3701508   |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 299/100000 [06:38<35:42:09,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000003000_steps\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0046337363 |\n",
      "| clipfrac           | 0.05078125   |\n",
      "| explained_variance | 0.00742      |\n",
      "| fps                | 934          |\n",
      "| n_updates          | 300          |\n",
      "| policy_entropy     | 0.27129057   |\n",
      "| policy_loss        | 0.0023593248 |\n",
      "| serial_timesteps   | 38400        |\n",
      "| time_elapsed       | 399          |\n",
      "| total_timesteps    | 3000         |\n",
      "| value_loss         | 9.687841     |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 399/100000 [08:48<36:12:26,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000004000_steps\n",
      "--------------------------------------\n",
      "| approxkl           | 0.0051837093  |\n",
      "| clipfrac           | 0.0640625     |\n",
      "| explained_variance | -0.0659       |\n",
      "| fps                | 966           |\n",
      "| n_updates          | 400           |\n",
      "| policy_entropy     | 0.24144296    |\n",
      "| policy_loss        | 0.00013390342 |\n",
      "| serial_timesteps   | 51200         |\n",
      "| time_elapsed       | 529           |\n",
      "| total_timesteps    | 4000          |\n",
      "| value_loss         | 1.0669178     |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 499/100000 [10:58<36:06:58,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000005000_steps\n",
      "---------------------------------------\n",
      "| approxkl           | 0.002082266    |\n",
      "| clipfrac           | 0.02890625     |\n",
      "| explained_variance | 0.382          |\n",
      "| fps                | 975            |\n",
      "| n_updates          | 500            |\n",
      "| policy_entropy     | 0.19474748     |\n",
      "| policy_loss        | -0.00021836255 |\n",
      "| serial_timesteps   | 64000          |\n",
      "| time_elapsed       | 659            |\n",
      "| total_timesteps    | 5000           |\n",
      "| value_loss         | 3.5456357      |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 599/100000 [13:08<35:24:13,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000006000_steps\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0122581655 |\n",
      "| clipfrac           | 0.0828125    |\n",
      "| explained_variance | 0.267        |\n",
      "| fps                | 921          |\n",
      "| n_updates          | 600          |\n",
      "| policy_entropy     | 0.20603713   |\n",
      "| policy_loss        | 0.005957109  |\n",
      "| serial_timesteps   | 76800        |\n",
      "| time_elapsed       | 788          |\n",
      "| total_timesteps    | 6000         |\n",
      "| value_loss         | 75.29913     |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 699/100000 [15:16<35:25:08,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000007000_steps\n",
      "------------------------------------\n",
      "| approxkl           | 0.003053822 |\n",
      "| clipfrac           | 0.0265625   |\n",
      "| explained_variance | 0.665       |\n",
      "| fps                | 963         |\n",
      "| n_updates          | 700         |\n",
      "| policy_entropy     | 0.113891944 |\n",
      "| policy_loss        | 0.003999182 |\n",
      "| serial_timesteps   | 89600       |\n",
      "| time_elapsed       | 917         |\n",
      "| total_timesteps    | 7000        |\n",
      "| value_loss         | 1.06994     |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 799/100000 [17:25<35:26:53,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000008000_steps\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0021551773 |\n",
      "| clipfrac           | 0.02421875   |\n",
      "| explained_variance | 0.284        |\n",
      "| fps                | 983          |\n",
      "| n_updates          | 800          |\n",
      "| policy_entropy     | 0.11800218   |\n",
      "| policy_loss        | 0.0015485018 |\n",
      "| serial_timesteps   | 102400       |\n",
      "| time_elapsed       | 1.05e+03     |\n",
      "| total_timesteps    | 8000         |\n",
      "| value_loss         | 5.085807     |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 899/100000 [19:33<35:13:42,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000009000_steps\n",
      "------------------------------------\n",
      "| approxkl           | 0.003321585 |\n",
      "| clipfrac           | 0.02421875  |\n",
      "| explained_variance | 0.186       |\n",
      "| fps                | 990         |\n",
      "| n_updates          | 900         |\n",
      "| policy_entropy     | 0.08639772  |\n",
      "| policy_loss        | -0.00334267 |\n",
      "| serial_timesteps   | 115200      |\n",
      "| time_elapsed       | 1.17e+03    |\n",
      "| total_timesteps    | 9000        |\n",
      "| value_loss         | 1.5316322   |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 999/100000 [21:37<33:46:32,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000010000_steps\n",
      ">>> log_path >>>\n",
      "./logs/results/evaluations\n",
      "./logs/results/evaluations-000010000\n",
      ">>> episode_rewards >>>\n",
      "[array([-15.790001], dtype=float32), array([-15.790001], dtype=float32), array([-15.790001], dtype=float32), array([-15.790001], dtype=float32), array([-15.790001], dtype=float32)]\n",
      "Eval num_timesteps=10000, episode_reward=-15.79 +/- 0.00\n",
      "Episode length: 2316.00 +/- 0.00\n",
      "New best mean reward!\n",
      "--------------------------------------\n",
      "| approxkl           | 0.0038850687  |\n",
      "| clipfrac           | 0.02578125    |\n",
      "| explained_variance | 0.889         |\n",
      "| fps                | 56            |\n",
      "| n_updates          | 1000          |\n",
      "| policy_entropy     | 0.07517873    |\n",
      "| policy_loss        | -0.0016102504 |\n",
      "| serial_timesteps   | 128000        |\n",
      "| time_elapsed       | 1.3e+03       |\n",
      "| total_timesteps    | 10000         |\n",
      "| value_loss         | 0.32060057    |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1099/100000 [24:02<34:36:50,  1.26s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000011000_steps\n",
      "---------------------------------------\n",
      "| approxkl           | 0.0038266699   |\n",
      "| clipfrac           | 0.03203125     |\n",
      "| explained_variance | 0.35           |\n",
      "| fps                | 1009           |\n",
      "| n_updates          | 1100           |\n",
      "| policy_entropy     | 0.08701055     |\n",
      "| policy_loss        | -0.00013081785 |\n",
      "| serial_timesteps   | 140800         |\n",
      "| time_elapsed       | 1.44e+03       |\n",
      "| total_timesteps    | 11000          |\n",
      "| value_loss         | 0.9032391      |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1199/100000 [26:13<34:19:18,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000012000_steps\n",
      "-------------------------------------\n",
      "| approxkl           | 0.011766444  |\n",
      "| clipfrac           | 0.05         |\n",
      "| explained_variance | 0.371        |\n",
      "| fps                | 985          |\n",
      "| n_updates          | 1200         |\n",
      "| policy_entropy     | 0.12119152   |\n",
      "| policy_loss        | 0.0015398883 |\n",
      "| serial_timesteps   | 153600       |\n",
      "| time_elapsed       | 1.57e+03     |\n",
      "| total_timesteps    | 12000        |\n",
      "| value_loss         | 15.9847355   |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1299/100000 [28:16<33:19:04,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000013000_steps\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0035392083 |\n",
      "| clipfrac           | 0.03515625   |\n",
      "| explained_variance | 0.688        |\n",
      "| fps                | 1040         |\n",
      "| n_updates          | 1300         |\n",
      "| policy_entropy     | 0.0807072    |\n",
      "| policy_loss        | 0.0053879092 |\n",
      "| serial_timesteps   | 166400       |\n",
      "| time_elapsed       | 1.7e+03      |\n",
      "| total_timesteps    | 13000        |\n",
      "| value_loss         | 2.8037403    |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1399/100000 [30:18<33:30:07,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000014000_steps\n",
      "-------------------------------------\n",
      "| approxkl           | 0.011101982  |\n",
      "| clipfrac           | 0.06484375   |\n",
      "| explained_variance | 0.639        |\n",
      "| fps                | 1042         |\n",
      "| n_updates          | 1400         |\n",
      "| policy_entropy     | 0.14903775   |\n",
      "| policy_loss        | 0.0034557295 |\n",
      "| serial_timesteps   | 179200       |\n",
      "| time_elapsed       | 1.82e+03     |\n",
      "| total_timesteps    | 14000        |\n",
      "| value_loss         | 2.6775548    |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 1499/100000 [32:20<32:58:09,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000015000_steps\n",
      "---------------------------------------\n",
      "| approxkl           | 0.002569687    |\n",
      "| clipfrac           | 0.0140625      |\n",
      "| explained_variance | 0.627          |\n",
      "| fps                | 1035           |\n",
      "| n_updates          | 1500           |\n",
      "| policy_entropy     | 0.047458183    |\n",
      "| policy_loss        | -0.00028750906 |\n",
      "| serial_timesteps   | 192000         |\n",
      "| time_elapsed       | 1.94e+03       |\n",
      "| total_timesteps    | 15000          |\n",
      "| value_loss         | 1.0755298      |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1599/100000 [34:23<33:02:56,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000016000_steps\n",
      "-------------------------------------\n",
      "| approxkl           | 0.007587923  |\n",
      "| clipfrac           | 0.0484375    |\n",
      "| explained_variance | 0.481        |\n",
      "| fps                | 975          |\n",
      "| n_updates          | 1600         |\n",
      "| policy_entropy     | 0.10268203   |\n",
      "| policy_loss        | 0.0031424924 |\n",
      "| serial_timesteps   | 204800       |\n",
      "| time_elapsed       | 2.06e+03     |\n",
      "| total_timesteps    | 16000        |\n",
      "| value_loss         | 40.32499     |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1699/100000 [36:25<33:36:48,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000017000_steps\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0018908646 |\n",
      "| clipfrac           | 0.02421875   |\n",
      "| explained_variance | 0.73         |\n",
      "| fps                | 1038         |\n",
      "| n_updates          | 1700         |\n",
      "| policy_entropy     | 0.1384643    |\n",
      "| policy_loss        | 0.0004255073 |\n",
      "| serial_timesteps   | 217600       |\n",
      "| time_elapsed       | 2.19e+03     |\n",
      "| total_timesteps    | 17000        |\n",
      "| value_loss         | 5.3183875    |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1799/100000 [38:27<33:27:59,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000018000_steps\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0063681095 |\n",
      "| clipfrac           | 0.03671875   |\n",
      "| explained_variance | 0.896        |\n",
      "| fps                | 977          |\n",
      "| n_updates          | 1800         |\n",
      "| policy_entropy     | 0.080637746  |\n",
      "| policy_loss        | 0.0002947243 |\n",
      "| serial_timesteps   | 230400       |\n",
      "| time_elapsed       | 2.31e+03     |\n",
      "| total_timesteps    | 18000        |\n",
      "| value_loss         | 0.36299616   |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1899/100000 [40:30<33:03:46,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000019000_steps\n",
      "--------------------------------------\n",
      "| approxkl           | 0.0019885828  |\n",
      "| clipfrac           | 0.01953125    |\n",
      "| explained_variance | 0.507         |\n",
      "| fps                | 993           |\n",
      "| n_updates          | 1900          |\n",
      "| policy_entropy     | 0.14090213    |\n",
      "| policy_loss        | -0.0036341778 |\n",
      "| serial_timesteps   | 243200        |\n",
      "| time_elapsed       | 2.43e+03      |\n",
      "| total_timesteps    | 19000         |\n",
      "| value_loss         | 5.3168664     |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 1999/100000 [42:32<33:09:17,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000020000_steps\n",
      ">>> log_path >>>\n",
      "./logs/results/evaluations\n",
      "./logs/results/evaluations-000020000\n",
      ">>> episode_rewards >>>\n",
      "[array([-43.909954], dtype=float32), array([-43.909954], dtype=float32), array([-43.909954], dtype=float32), array([-43.909954], dtype=float32), array([-43.909954], dtype=float32)]\n",
      "Eval num_timesteps=20000, episode_reward=-43.91 +/- 0.00\n",
      "Episode length: 2316.00 +/- 0.00\n",
      "-------------------------------------\n",
      "| approxkl           | 0.005457228  |\n",
      "| clipfrac           | 0.0328125    |\n",
      "| explained_variance | 0.594        |\n",
      "| fps                | 56           |\n",
      "| n_updates          | 2000         |\n",
      "| policy_entropy     | 0.08803544   |\n",
      "| policy_loss        | -0.001434909 |\n",
      "| serial_timesteps   | 256000       |\n",
      "| time_elapsed       | 2.55e+03     |\n",
      "| total_timesteps    | 20000        |\n",
      "| value_loss         | 0.9053043    |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2099/100000 [44:55<33:33:07,  1.23s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000021000_steps\n",
      "-------------------------------------\n",
      "| approxkl           | 0.00818572   |\n",
      "| clipfrac           | 0.0578125    |\n",
      "| explained_variance | 0.924        |\n",
      "| fps                | 1006         |\n",
      "| n_updates          | 2100         |\n",
      "| policy_entropy     | 0.08793589   |\n",
      "| policy_loss        | 0.0017763147 |\n",
      "| serial_timesteps   | 268800       |\n",
      "| time_elapsed       | 2.7e+03      |\n",
      "| total_timesteps    | 21000        |\n",
      "| value_loss         | 0.6832622    |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2199/100000 [46:59<33:31:21,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000022000_steps\n",
      "---------------------------------------\n",
      "| approxkl           | 0.0028336255   |\n",
      "| clipfrac           | 0.03359375     |\n",
      "| explained_variance | 0.717          |\n",
      "| fps                | 1044           |\n",
      "| n_updates          | 2200           |\n",
      "| policy_entropy     | 0.21093182     |\n",
      "| policy_loss        | -0.00017717518 |\n",
      "| serial_timesteps   | 281600         |\n",
      "| time_elapsed       | 2.82e+03       |\n",
      "| total_timesteps    | 22000          |\n",
      "| value_loss         | 95.41115       |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2299/100000 [49:01<33:43:24,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000023000_steps\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0056657656 |\n",
      "| clipfrac           | 0.01875      |\n",
      "| explained_variance | 0.727        |\n",
      "| fps                | 1005         |\n",
      "| n_updates          | 2300         |\n",
      "| policy_entropy     | 0.06707475   |\n",
      "| policy_loss        | 0.006640022  |\n",
      "| serial_timesteps   | 294400       |\n",
      "| time_elapsed       | 2.94e+03     |\n",
      "| total_timesteps    | 23000        |\n",
      "| value_loss         | 2.3141267    |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2399/100000 [51:04<33:05:08,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000024000_steps\n",
      "--------------------------------------\n",
      "| approxkl           | 0.0035434682  |\n",
      "| clipfrac           | 0.0296875     |\n",
      "| explained_variance | 0.797         |\n",
      "| fps                | 887           |\n",
      "| n_updates          | 2400          |\n",
      "| policy_entropy     | 0.16897787    |\n",
      "| policy_loss        | -0.0028362258 |\n",
      "| serial_timesteps   | 307200        |\n",
      "| time_elapsed       | 3.06e+03      |\n",
      "| total_timesteps    | 24000         |\n",
      "| value_loss         | 1.9587133     |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2499/100000 [53:07<34:46:36,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000025000_steps\n",
      "--------------------------------------\n",
      "| approxkl           | 0.0018932298  |\n",
      "| clipfrac           | 0.025         |\n",
      "| explained_variance | 0.807         |\n",
      "| fps                | 754           |\n",
      "| n_updates          | 2500          |\n",
      "| policy_entropy     | 0.07365663    |\n",
      "| policy_loss        | -0.0005963657 |\n",
      "| serial_timesteps   | 320000        |\n",
      "| time_elapsed       | 3.19e+03      |\n",
      "| total_timesteps    | 25000         |\n",
      "| value_loss         | 1.5980707     |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2599/100000 [55:10<33:27:05,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000026000_steps\n",
      "------------------------------------\n",
      "| approxkl           | 0.007674123 |\n",
      "| clipfrac           | 0.03125     |\n",
      "| explained_variance | 0.895       |\n",
      "| fps                | 993         |\n",
      "| n_updates          | 2600        |\n",
      "| policy_entropy     | 0.06391017  |\n",
      "| policy_loss        | 0.011273252 |\n",
      "| serial_timesteps   | 332800      |\n",
      "| time_elapsed       | 3.31e+03    |\n",
      "| total_timesteps    | 26000       |\n",
      "| value_loss         | 0.6559852   |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2699/100000 [57:12<32:38:10,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000027000_steps\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0034306683 |\n",
      "| clipfrac           | 0.01953125   |\n",
      "| explained_variance | 0.898        |\n",
      "| fps                | 993          |\n",
      "| n_updates          | 2700         |\n",
      "| policy_entropy     | 0.06987616   |\n",
      "| policy_loss        | -0.00120079  |\n",
      "| serial_timesteps   | 345600       |\n",
      "| time_elapsed       | 3.43e+03     |\n",
      "| total_timesteps    | 27000        |\n",
      "| value_loss         | 0.19021837   |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2799/100000 [59:15<32:48:17,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000028000_steps\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0011275433 |\n",
      "| clipfrac           | 0.0140625    |\n",
      "| explained_variance | 0.73         |\n",
      "| fps                | 1009         |\n",
      "| n_updates          | 2800         |\n",
      "| policy_entropy     | 0.091271706  |\n",
      "| policy_loss        | 0.0015123618 |\n",
      "| serial_timesteps   | 358400       |\n",
      "| time_elapsed       | 3.56e+03     |\n",
      "| total_timesteps    | 28000        |\n",
      "| value_loss         | 19.579681    |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2899/100000 [1:01:18<32:36:42,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000029000_steps\n",
      "------------------------------------\n",
      "| approxkl           | 0.006731367 |\n",
      "| clipfrac           | 0.02734375  |\n",
      "| explained_variance | 0.776       |\n",
      "| fps                | 1045        |\n",
      "| n_updates          | 2900        |\n",
      "| policy_entropy     | 0.061595656 |\n",
      "| policy_loss        | 0.005712979 |\n",
      "| serial_timesteps   | 371200      |\n",
      "| time_elapsed       | 3.68e+03    |\n",
      "| total_timesteps    | 29000       |\n",
      "| value_loss         | 5.627838    |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 2999/100000 [1:03:20<32:19:46,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000030000_steps\n",
      ">>> log_path >>>\n",
      "./logs/results/evaluations\n",
      "./logs/results/evaluations-000030000\n",
      ">>> episode_rewards >>>\n",
      "[array([-3.689998], dtype=float32), array([-3.689998], dtype=float32), array([-3.689998], dtype=float32), array([-3.689998], dtype=float32), array([-3.689998], dtype=float32)]\n",
      "Eval num_timesteps=30000, episode_reward=-3.69 +/- 0.00\n",
      "Episode length: 2316.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0071242363 |\n",
      "| clipfrac           | 0.0703125    |\n",
      "| explained_variance | 0.543        |\n",
      "| fps                | 56           |\n",
      "| n_updates          | 3000         |\n",
      "| policy_entropy     | 0.18335584   |\n",
      "| policy_loss        | 0.0009444929 |\n",
      "| serial_timesteps   | 384000       |\n",
      "| time_elapsed       | 3.8e+03      |\n",
      "| total_timesteps    | 30000        |\n",
      "| value_loss         | 1.4452789    |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3099/100000 [1:05:44<33:06:11,  1.23s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000031000_steps\n",
      "---------------------------------------\n",
      "| approxkl           | 0.0006456541   |\n",
      "| clipfrac           | 0.0046875      |\n",
      "| explained_variance | 0.931          |\n",
      "| fps                | 946            |\n",
      "| n_updates          | 3100           |\n",
      "| policy_entropy     | 0.032284934    |\n",
      "| policy_loss        | -0.00049979443 |\n",
      "| serial_timesteps   | 396800         |\n",
      "| time_elapsed       | 3.94e+03       |\n",
      "| total_timesteps    | 31000          |\n",
      "| value_loss         | 0.44402367     |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3199/100000 [1:07:47<32:57:14,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000032000_steps\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0009673289 |\n",
      "| clipfrac           | 0.0109375    |\n",
      "| explained_variance | 0.722        |\n",
      "| fps                | 1025         |\n",
      "| n_updates          | 3200         |\n",
      "| policy_entropy     | 0.09744081   |\n",
      "| policy_loss        | 0.00130873   |\n",
      "| serial_timesteps   | 409600       |\n",
      "| time_elapsed       | 4.07e+03     |\n",
      "| total_timesteps    | 32000        |\n",
      "| value_loss         | 44.191948    |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3299/100000 [1:09:50<33:02:59,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000033000_steps\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0010837342 |\n",
      "| clipfrac           | 0.01796875   |\n",
      "| explained_variance | 0.88         |\n",
      "| fps                | 1034         |\n",
      "| n_updates          | 3300         |\n",
      "| policy_entropy     | 0.101809144  |\n",
      "| policy_loss        | 0.0013548869 |\n",
      "| serial_timesteps   | 422400       |\n",
      "| time_elapsed       | 4.19e+03     |\n",
      "| total_timesteps    | 33000        |\n",
      "| value_loss         | 3.757803     |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3399/100000 [1:11:52<32:37:52,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000034000_steps\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0025553901 |\n",
      "| clipfrac           | 0.0140625    |\n",
      "| explained_variance | 0.96         |\n",
      "| fps                | 979          |\n",
      "| n_updates          | 3400         |\n",
      "| policy_entropy     | 0.039774872  |\n",
      "| policy_loss        | 0.006609178  |\n",
      "| serial_timesteps   | 435200       |\n",
      "| time_elapsed       | 4.31e+03     |\n",
      "| total_timesteps    | 34000        |\n",
      "| value_loss         | 0.50142616   |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3499/100000 [1:13:55<33:03:58,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000035000_steps\n",
      "--------------------------------------\n",
      "| approxkl           | 0.0016512986  |\n",
      "| clipfrac           | 0.0203125     |\n",
      "| explained_variance | 0.772         |\n",
      "| fps                | 945           |\n",
      "| n_updates          | 3500          |\n",
      "| policy_entropy     | 0.13151282    |\n",
      "| policy_loss        | -0.0011440315 |\n",
      "| serial_timesteps   | 448000        |\n",
      "| time_elapsed       | 4.44e+03      |\n",
      "| total_timesteps    | 35000         |\n",
      "| value_loss         | 5.7499266     |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 3599/100000 [1:15:58<33:28:12,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000036000_steps\n",
      "--------------------------------------\n",
      "| approxkl           | 0.00012557182 |\n",
      "| clipfrac           | 0.003125      |\n",
      "| explained_variance | 0.626         |\n",
      "| fps                | 1003          |\n",
      "| n_updates          | 3600          |\n",
      "| policy_entropy     | 0.022844788   |\n",
      "| policy_loss        | -0.0014682328 |\n",
      "| serial_timesteps   | 460800        |\n",
      "| time_elapsed       | 4.56e+03      |\n",
      "| total_timesteps    | 36000         |\n",
      "| value_loss         | 1.4905992     |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 3699/100000 [1:18:01<34:07:53,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000037000_steps\n",
      "--------------------------------------\n",
      "| approxkl           | 0.0060702767  |\n",
      "| clipfrac           | 0.01796875    |\n",
      "| explained_variance | 0.976         |\n",
      "| fps                | 1026          |\n",
      "| n_updates          | 3700          |\n",
      "| policy_entropy     | 0.055525053   |\n",
      "| policy_loss        | 0.00086890656 |\n",
      "| serial_timesteps   | 473600        |\n",
      "| time_elapsed       | 4.68e+03      |\n",
      "| total_timesteps    | 37000         |\n",
      "| value_loss         | 0.5015337     |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 3799/100000 [1:20:03<33:05:44,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000038000_steps\n",
      "--------------------------------------\n",
      "| approxkl           | 0.0129872095  |\n",
      "| clipfrac           | 0.0453125     |\n",
      "| explained_variance | 0.821         |\n",
      "| fps                | 1040          |\n",
      "| n_updates          | 3800          |\n",
      "| policy_entropy     | 0.13422236    |\n",
      "| policy_loss        | 0.00036537554 |\n",
      "| serial_timesteps   | 486400        |\n",
      "| time_elapsed       | 4.8e+03       |\n",
      "| total_timesteps    | 38000         |\n",
      "| value_loss         | 97.36544      |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 3899/100000 [1:22:05<32:28:38,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000039000_steps\n",
      "--------------------------------------\n",
      "| approxkl           | 0.009641727   |\n",
      "| clipfrac           | 0.0109375     |\n",
      "| explained_variance | 0.661         |\n",
      "| fps                | 1040          |\n",
      "| n_updates          | 3900          |\n",
      "| policy_entropy     | 0.022852536   |\n",
      "| policy_loss        | -0.0005151468 |\n",
      "| serial_timesteps   | 499200        |\n",
      "| time_elapsed       | 4.93e+03      |\n",
      "| total_timesteps    | 39000         |\n",
      "| value_loss         | 2.5965228     |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 3999/100000 [1:24:09<34:22:15,  1.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000040000_steps\n",
      ">>> log_path >>>\n",
      "./logs/results/evaluations\n",
      "./logs/results/evaluations-000040000\n",
      ">>> episode_rewards >>>\n",
      "[array([43.81005], dtype=float32), array([43.81005], dtype=float32), array([43.81005], dtype=float32), array([43.81005], dtype=float32), array([43.81005], dtype=float32)]\n",
      "Eval num_timesteps=40000, episode_reward=43.81 +/- 0.00\n",
      "Episode length: 2316.00 +/- 0.00\n",
      "New best mean reward!\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0026972485 |\n",
      "| clipfrac           | 0.0234375    |\n",
      "| explained_variance | 0.891        |\n",
      "| fps                | 56           |\n",
      "| n_updates          | 4000         |\n",
      "| policy_entropy     | 0.095674016  |\n",
      "| policy_loss        | 0.0019044044 |\n",
      "| serial_timesteps   | 512000       |\n",
      "| time_elapsed       | 5.05e+03     |\n",
      "| total_timesteps    | 40000        |\n",
      "| value_loss         | 0.76276934   |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4099/100000 [1:26:32<32:30:18,  1.22s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000041000_steps\n",
      "--------------------------------------\n",
      "| approxkl           | 0.0037198558  |\n",
      "| clipfrac           | 0.02109375    |\n",
      "| explained_variance | 0.451         |\n",
      "| fps                | 1031          |\n",
      "| n_updates          | 4100          |\n",
      "| policy_entropy     | 0.06480634    |\n",
      "| policy_loss        | 4.6763707e-05 |\n",
      "| serial_timesteps   | 524800        |\n",
      "| time_elapsed       | 5.19e+03      |\n",
      "| total_timesteps    | 41000         |\n",
      "| value_loss         | 2.1075585     |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4199/100000 [1:28:34<31:55:55,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000042000_steps\n",
      "--------------------------------------\n",
      "| approxkl           | 0.00023534216 |\n",
      "| clipfrac           | 0.0015625     |\n",
      "| explained_variance | 0.809         |\n",
      "| fps                | 1058          |\n",
      "| n_updates          | 4200          |\n",
      "| policy_entropy     | 0.020233002   |\n",
      "| policy_loss        | 0.00012511239 |\n",
      "| serial_timesteps   | 537600        |\n",
      "| time_elapsed       | 5.31e+03      |\n",
      "| total_timesteps    | 42000         |\n",
      "| value_loss         | 2.8146644     |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4299/100000 [1:30:37<33:11:36,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000043000_steps\n",
      "---------------------------------------\n",
      "| approxkl           | 0.0018390762   |\n",
      "| clipfrac           | 0.0140625      |\n",
      "| explained_variance | 0.666          |\n",
      "| fps                | 970            |\n",
      "| n_updates          | 4300           |\n",
      "| policy_entropy     | 0.051222585    |\n",
      "| policy_loss        | -0.00036888634 |\n",
      "| serial_timesteps   | 550400         |\n",
      "| time_elapsed       | 5.44e+03       |\n",
      "| total_timesteps    | 43000          |\n",
      "| value_loss         | 0.49681234     |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4399/100000 [1:32:39<32:31:02,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000044000_steps\n",
      "--------------------------------------\n",
      "| approxkl           | 0.0018511482  |\n",
      "| clipfrac           | 0.028125      |\n",
      "| explained_variance | 0.766         |\n",
      "| fps                | 996           |\n",
      "| n_updates          | 4400          |\n",
      "| policy_entropy     | 0.120406285   |\n",
      "| policy_loss        | -0.0005991949 |\n",
      "| serial_timesteps   | 563200        |\n",
      "| time_elapsed       | 5.56e+03      |\n",
      "| total_timesteps    | 44000         |\n",
      "| value_loss         | 29.00235      |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4499/100000 [1:34:42<32:22:31,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000045000_steps\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0064255176 |\n",
      "| clipfrac           | 0.02734375   |\n",
      "| explained_variance | 0.88         |\n",
      "| fps                | 1036         |\n",
      "| n_updates          | 4500         |\n",
      "| policy_entropy     | 0.054881215  |\n",
      "| policy_loss        | 0.007325108  |\n",
      "| serial_timesteps   | 576000       |\n",
      "| time_elapsed       | 5.68e+03     |\n",
      "| total_timesteps    | 45000        |\n",
      "| value_loss         | 2.8631213    |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 4599/100000 [1:36:44<32:35:24,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000046000_steps\n",
      "--------------------------------------\n",
      "| approxkl           | 0.0037338356  |\n",
      "| clipfrac           | 0.0328125     |\n",
      "| explained_variance | 0.938         |\n",
      "| fps                | 993           |\n",
      "| n_updates          | 4600          |\n",
      "| policy_entropy     | 0.118865296   |\n",
      "| policy_loss        | -0.0061758775 |\n",
      "| serial_timesteps   | 588800        |\n",
      "| time_elapsed       | 5.8e+03       |\n",
      "| total_timesteps    | 46000         |\n",
      "| value_loss         | 1.2138989     |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 4699/100000 [1:38:46<32:14:29,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000047000_steps\n",
      "--------------------------------------\n",
      "| approxkl           | 0.0036001317  |\n",
      "| clipfrac           | 0.01171875    |\n",
      "| explained_variance | 0.933         |\n",
      "| fps                | 1028          |\n",
      "| n_updates          | 4700          |\n",
      "| policy_entropy     | 0.028604722   |\n",
      "| policy_loss        | -0.0018474646 |\n",
      "| serial_timesteps   | 601600        |\n",
      "| time_elapsed       | 5.93e+03      |\n",
      "| total_timesteps    | 47000         |\n",
      "| value_loss         | 0.29214376    |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 4799/100000 [1:40:49<31:52:19,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000048000_steps\n",
      "--------------------------------------\n",
      "| approxkl           | 0.00057010585 |\n",
      "| clipfrac           | 0.00703125    |\n",
      "| explained_variance | 0.682         |\n",
      "| fps                | 1044          |\n",
      "| n_updates          | 4800          |\n",
      "| policy_entropy     | 0.113285914   |\n",
      "| policy_loss        | 0.0011494962  |\n",
      "| serial_timesteps   | 614400        |\n",
      "| time_elapsed       | 6.05e+03      |\n",
      "| total_timesteps    | 48000         |\n",
      "| value_loss         | 19.194555     |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 4899/100000 [1:42:51<33:00:30,  1.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000049000_steps\n",
      "--------------------------------------\n",
      "| approxkl           | 0.00035552584 |\n",
      "| clipfrac           | 0.00390625    |\n",
      "| explained_variance | 0.626         |\n",
      "| fps                | 1029          |\n",
      "| n_updates          | 4900          |\n",
      "| policy_entropy     | 0.070897326   |\n",
      "| policy_loss        | 0.00060239807 |\n",
      "| serial_timesteps   | 627200        |\n",
      "| time_elapsed       | 6.17e+03      |\n",
      "| total_timesteps    | 49000         |\n",
      "| value_loss         | 4.77629       |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▍         | 4999/100000 [1:44:53<32:47:05,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000050000_steps\n",
      ">>> log_path >>>\n",
      "./logs/results/evaluations\n",
      "./logs/results/evaluations-000050000\n",
      ">>> episode_rewards >>>\n",
      "[array([8.409998], dtype=float32), array([8.409998], dtype=float32), array([8.409998], dtype=float32), array([8.409998], dtype=float32), array([8.409998], dtype=float32)]\n",
      "Eval num_timesteps=50000, episode_reward=8.41 +/- 0.00\n",
      "Episode length: 2316.00 +/- 0.00\n",
      "--------------------------------------\n",
      "| approxkl           | 0.001788894   |\n",
      "| clipfrac           | 0.00546875    |\n",
      "| explained_variance | 0.968         |\n",
      "| fps                | 56            |\n",
      "| n_updates          | 5000          |\n",
      "| policy_entropy     | 0.014058528   |\n",
      "| policy_loss        | 0.00090261846 |\n",
      "| serial_timesteps   | 640000        |\n",
      "| time_elapsed       | 6.29e+03      |\n",
      "| total_timesteps    | 50000         |\n",
      "| value_loss         | 0.19789083    |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5099/100000 [1:47:17<32:27:33,  1.23s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000051000_steps\n",
      "--------------------------------------\n",
      "| approxkl           | 0.0002122804  |\n",
      "| clipfrac           | 0.00078125    |\n",
      "| explained_variance | 0.819         |\n",
      "| fps                | 932           |\n",
      "| n_updates          | 5100          |\n",
      "| policy_entropy     | 0.11266794    |\n",
      "| policy_loss        | 3.4893677e-05 |\n",
      "| serial_timesteps   | 652800        |\n",
      "| time_elapsed       | 6.44e+03      |\n",
      "| total_timesteps    | 51000         |\n",
      "| value_loss         | 13.335446     |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5199/100000 [1:49:20<32:16:59,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000052000_steps\n",
      "--------------------------------------\n",
      "| approxkl           | 0.00081556046 |\n",
      "| clipfrac           | 0.0046875     |\n",
      "| explained_variance | 0.178         |\n",
      "| fps                | 1030          |\n",
      "| n_updates          | 5200          |\n",
      "| policy_entropy     | 0.016506923   |\n",
      "| policy_loss        | 0.0015283142  |\n",
      "| serial_timesteps   | 665600        |\n",
      "| time_elapsed       | 6.56e+03      |\n",
      "| total_timesteps    | 52000         |\n",
      "| value_loss         | 2.0694275     |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5299/100000 [1:51:22<32:04:25,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000053000_steps\n",
      "-------------------------------------\n",
      "| approxkl           | 0.000580385  |\n",
      "| clipfrac           | 0.01015625   |\n",
      "| explained_variance | 0.935        |\n",
      "| fps                | 947          |\n",
      "| n_updates          | 5300         |\n",
      "| policy_entropy     | 0.04731689   |\n",
      "| policy_loss        | 0.0013980482 |\n",
      "| serial_timesteps   | 678400       |\n",
      "| time_elapsed       | 6.68e+03     |\n",
      "| total_timesteps    | 53000        |\n",
      "| value_loss         | 1.0951936    |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5399/100000 [1:53:25<31:56:04,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000054000_steps\n",
      "-------------------------------------\n",
      "| approxkl           | 0.0033432997 |\n",
      "| clipfrac           | 0.01640625   |\n",
      "| explained_variance | 0.971        |\n",
      "| fps                | 1041         |\n",
      "| n_updates          | 5400         |\n",
      "| policy_entropy     | 0.059239846  |\n",
      "| policy_loss        | 0.0022412506 |\n",
      "| serial_timesteps   | 691200       |\n",
      "| time_elapsed       | 6.81e+03     |\n",
      "| total_timesteps    | 54000        |\n",
      "| value_loss         | 80.16962     |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5499/100000 [1:55:28<31:49:14,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000055000_steps\n",
      "--------------------------------------\n",
      "| approxkl           | 0.01697627    |\n",
      "| clipfrac           | 0.0109375     |\n",
      "| explained_variance | 0.811         |\n",
      "| fps                | 981           |\n",
      "| n_updates          | 5500          |\n",
      "| policy_entropy     | 0.017614922   |\n",
      "| policy_loss        | 0.00071982056 |\n",
      "| serial_timesteps   | 704000        |\n",
      "| time_elapsed       | 6.93e+03      |\n",
      "| total_timesteps    | 55000         |\n",
      "| value_loss         | 2.081551      |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 5599/100000 [1:57:30<32:18:38,  1.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000056000_steps\n",
      "---------------------------------------\n",
      "| approxkl           | 0.00013984609  |\n",
      "| clipfrac           | 0.00078125     |\n",
      "| explained_variance | 0.952          |\n",
      "| fps                | 954            |\n",
      "| n_updates          | 5600           |\n",
      "| policy_entropy     | 0.05645321     |\n",
      "| policy_loss        | -0.00010707769 |\n",
      "| serial_timesteps   | 716800         |\n",
      "| time_elapsed       | 7.05e+03       |\n",
      "| total_timesteps    | 56000          |\n",
      "| value_loss         | 1.4837444      |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 5699/100000 [1:59:32<31:52:08,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000057000_steps\n",
      "--------------------------------------\n",
      "| approxkl           | 0.0010767195  |\n",
      "| clipfrac           | 0.0078125     |\n",
      "| explained_variance | 0.706         |\n",
      "| fps                | 1033          |\n",
      "| n_updates          | 5700          |\n",
      "| policy_entropy     | 0.05166955    |\n",
      "| policy_loss        | 0.00015347516 |\n",
      "| serial_timesteps   | 729600        |\n",
      "| time_elapsed       | 7.17e+03      |\n",
      "| total_timesteps    | 57000         |\n",
      "| value_loss         | 0.9623081     |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 5799/100000 [2:01:34<32:59:02,  1.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000058000_steps\n",
      "---------------------------------------\n",
      "| approxkl           | 0.0023003414   |\n",
      "| clipfrac           | 0.00703125     |\n",
      "| explained_variance | 0.433          |\n",
      "| fps                | 1057           |\n",
      "| n_updates          | 5800           |\n",
      "| policy_entropy     | 0.032752864    |\n",
      "| policy_loss        | -0.00031542848 |\n",
      "| serial_timesteps   | 742400         |\n",
      "| time_elapsed       | 7.29e+03       |\n",
      "| total_timesteps    | 58000          |\n",
      "| value_loss         | 2.4711194      |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 5899/100000 [2:03:36<31:35:11,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000059000_steps\n",
      "--------------------------------------\n",
      "| approxkl           | 0.00013923098 |\n",
      "| clipfrac           | 0.00234375    |\n",
      "| explained_variance | 0.923         |\n",
      "| fps                | 913           |\n",
      "| n_updates          | 5900          |\n",
      "| policy_entropy     | 0.039015345   |\n",
      "| policy_loss        | 0.000315747   |\n",
      "| serial_timesteps   | 755200        |\n",
      "| time_elapsed       | 7.42e+03      |\n",
      "| total_timesteps    | 59000         |\n",
      "| value_loss         | 1.659661      |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 5999/100000 [2:05:39<32:21:34,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000060000_steps\n",
      ">>> log_path >>>\n",
      "./logs/results/evaluations\n",
      "./logs/results/evaluations-000060000\n",
      ">>> episode_rewards >>>\n",
      "[array([-40.750008], dtype=float32), array([-40.750008], dtype=float32), array([-40.750008], dtype=float32), array([-40.750008], dtype=float32), array([-40.750008], dtype=float32)]\n",
      "Eval num_timesteps=60000, episode_reward=-40.75 +/- 0.00\n",
      "Episode length: 2316.00 +/- 0.00\n",
      "------------------------------------\n",
      "| approxkl           | 0.028368104 |\n",
      "| clipfrac           | 0.0421875   |\n",
      "| explained_variance | 0.95        |\n",
      "| fps                | 56          |\n",
      "| n_updates          | 6000        |\n",
      "| policy_entropy     | 0.06931077  |\n",
      "| policy_loss        | 0.014651081 |\n",
      "| serial_timesteps   | 768000      |\n",
      "| time_elapsed       | 7.54e+03    |\n",
      "| total_timesteps    | 60000       |\n",
      "| value_loss         | 24.214462   |\n",
      "------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6099/100000 [2:08:03<31:51:00,  1.22s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000061000_steps\n",
      "-------------------------------------\n",
      "| approxkl           | 0.02616272   |\n",
      "| clipfrac           | 0.01015625   |\n",
      "| explained_variance | 0.95         |\n",
      "| fps                | 1044         |\n",
      "| n_updates          | 6100         |\n",
      "| policy_entropy     | 0.015914211  |\n",
      "| policy_loss        | 0.0009654627 |\n",
      "| serial_timesteps   | 780800       |\n",
      "| time_elapsed       | 7.68e+03     |\n",
      "| total_timesteps    | 61000        |\n",
      "| value_loss         | 0.35609126   |\n",
      "-------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6199/100000 [2:10:05<31:21:49,  1.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000062000_steps\n",
      "---------------------------------------\n",
      "| approxkl           | 0.0008042965   |\n",
      "| clipfrac           | 0.0078125      |\n",
      "| explained_variance | 0.962          |\n",
      "| fps                | 975            |\n",
      "| n_updates          | 6200           |\n",
      "| policy_entropy     | 0.060088076    |\n",
      "| policy_loss        | -0.00088717305 |\n",
      "| serial_timesteps   | 793600         |\n",
      "| time_elapsed       | 7.81e+03       |\n",
      "| total_timesteps    | 62000          |\n",
      "| value_loss         | 1.2462448      |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 6299/100000 [2:12:08<31:50:47,  1.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000063000_steps\n",
      "---------------------------------------\n",
      "| approxkl           | 6.396849e-05   |\n",
      "| clipfrac           | 0.0015625      |\n",
      "| explained_variance | 0.994          |\n",
      "| fps                | 966            |\n",
      "| n_updates          | 6300           |\n",
      "| policy_entropy     | 0.008996834    |\n",
      "| policy_loss        | -9.5124546e-05 |\n",
      "| serial_timesteps   | 806400         |\n",
      "| time_elapsed       | 7.93e+03       |\n",
      "| total_timesteps    | 63000          |\n",
      "| value_loss         | 0.21285756     |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 6399/100000 [2:14:12<32:09:17,  1.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./logs/PPO2_000000000_000064000_steps\n",
      "--------------------------------------\n",
      "| approxkl           | 0.0010036752  |\n",
      "| clipfrac           | 0.00546875    |\n",
      "| explained_variance | 0.824         |\n",
      "| fps                | 1040          |\n",
      "| n_updates          | 6400          |\n",
      "| policy_entropy     | 0.10376072    |\n",
      "| policy_loss        | 0.00050357974 |\n",
      "| serial_timesteps   | 819200        |\n",
      "| time_elapsed       | 8.05e+03      |\n",
      "| total_timesteps    | 64000         |\n",
      "| value_loss         | 27.062454     |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 6490/100000 [2:16:03<32:15:26,  1.24s/it]"
     ]
    }
   ],
   "source": [
    "#for i in tqdm(range(int(resume_idx),  5)):\n",
    "\n",
    "i = 0\n",
    "# ====================================================\n",
    "# callback\n",
    "#\n",
    "# -------\n",
    "# eval callback\n",
    "#\n",
    "eval_callback = EvalCallback2(env_marker_test2, best_model_save_path='./logs/best_model',\n",
    "                            log_path='./logs/results', eval_freq=eval_freq, verbose=2, name_prefix='PPO2_{:09d}_'.format(i))\n",
    "# ------0\n",
    "# checkpoint callback\n",
    "#\n",
    "checkpoint_callback = CheckpointCallback2(save_freq=save_freq, save_path='./logs/', name_prefix='PPO2_{:09d}'.format(i), verbose=2)\n",
    "\n",
    "# -------\n",
    "# merge callback\n",
    "#\n",
    "#callback = CallbackList([checkpoint_callback, eval_callback, ProgressBarCallback(total_timesteps)])\n",
    "callback = CallbackList([checkpoint_callback, eval_callback, ])\n",
    "\n",
    "_model_name, _resume_idx, _train_num, _resume_FLAG = get_latest_model_param()\n",
    "if(_resume_FLAG):\n",
    "    # ====================================================\n",
    "    # Model setting\n",
    "    #\n",
    "    model = PPO2.load(\"./logs/{}_{:09d}_{:09d}_steps\".format(_model_name, int(_resume_idx), int(_train_num)))\n",
    "    model.set_env(env)\n",
    "    model.train_num = _train_num\n",
    "\n",
    "    # -------\n",
    "    # tensorboard\n",
    "    #\n",
    "    model.tensorboard_log = \"logs\"\n",
    "    model.num_timesteps = _train_num\n",
    "\n",
    "#tb_log_name = \"sampleE{}v\".format(i)\n",
    "tb_log_name = \"AutoLoopTrain_no_feat\"\n",
    "print(\"tb_log_name : {}\".format(tb_log_name))\n",
    "\n",
    "model.learn(total_timesteps=total_timesteps , log_interval=100, tb_log_name=tb_log_name, reset_num_timesteps=False, callback=callback)\n",
    "\n",
    "# ====================================================\n",
    "# Save model\n",
    "#\n",
    "model.save(\"./logs/PPO2_{:09d}_{:09d}_steps\".format(i, total_timesteps*(i+1)*env_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
