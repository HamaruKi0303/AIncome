{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "AutoTrade_11.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hyyN-2qyK_T2"
      },
      "source": [
        "## 概要\n",
        "\n",
        "> Stable Baselines, a Fork of OpenAI Baselines - Training, Saving and Loading\n",
        "\n",
        "学習したモデルを保存して，それを読み込み再度学習を始めれる環境を作っていきます．\n",
        "\n",
        "Github Repo: [https://github.com/hill-a/stable-baselines](https://github.com/hill-a/stable-baselines)\n",
        "\n",
        "Medium article: [https://medium.com/@araffin/stable-baselines-a-fork-of-openai-baselines-df87c4b2fc82](https://medium.com/@araffin/stable-baselines-a-fork-of-openai-baselines-df87c4b2fc82)\n",
        "\n",
        ">[RL Baselines Zoo](https://github.com/araffin/rl-baselines-zoo) is a collection of pre-trained Reinforcement Learning agents using Stable-Baselines.\n",
        "\n",
        "RL Baselines Zooは、Stable-Baselinesを用いた強化学習エージェントをあらかじめ学習させたコレクションです。\n",
        "\n",
        ">It also provides basic scripts for training, evaluating agents, tuning hyperparameters and recording videos.\n",
        "\n",
        "また、トレーニング、エージェントの評価、ハイパーパラメータの調整、ビデオの録画のための基本的なスクリプトが用意されています。\n",
        "\n",
        "\n",
        "Documentation is available online: [https://stable-baselines.readthedocs.io/](https://stable-baselines.readthedocs.io/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## パッケージのインストール"
      ],
      "metadata": {
        "id": "jDsLa-J8fQqT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "公式のドキュメントは古く，そのままでは動かないため，下記のように少し修正します，"
      ],
      "metadata": {
        "id": "T72VlaPPfUnC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### OpenGym関係"
      ],
      "metadata": {
        "id": "WrofqIQ-e94W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ほんとに毎回Verに悩まされます"
      ],
      "metadata": {
        "id": "oADVHZuOfCz5"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWskDE2c9WoN"
      },
      "source": [
        "!pip install \"gym==0.19.0\"\n",
        "#!pip install stable-baselines[mpi]\n",
        "!apt install swig cmake libopenmpi-dev zlib1g-dev\n",
        "!pip install stable-baselines[mpi]==2.10.2 box2d box2d-kengz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### tensorflow"
      ],
      "metadata": {
        "id": "BUaVOX4ffPV7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall -y tensorflow-gpu\n",
        "!pip uninstall -y tensorflow\n",
        "!pip install tensorflow-gpu==1.14.0"
      ],
      "metadata": {
        "id": "5wV6FGJvfOGI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### tensorboard"
      ],
      "metadata": {
        "id": "5BOokqatffvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall tensorboard-plugin-wit --yes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CO2dgZfffgSl",
        "outputId": "b28fc030-090d-4cc5-af6c-2925820be29f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping tensorboard-plugin-wit as it is not installed.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JSso1Fscjw1E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FtY8FhliLsGm"
      },
      "source": [
        "## Import policy, RL agent, ..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BIedd7Pz9sOs"
      },
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "\n",
        "from stable_baselines import DQN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RapkYvTXL7Cd"
      },
      "source": [
        "## Create the Gym env and instantiate the agent\n",
        "\n",
        ">For this example, we will use Lunar Lander environment.\n",
        "\n",
        "この例では、Lunar Lander環境を使用します。\n",
        "\n",
        ">\"Landing outside landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt. Four discrete actions available: do nothing, fire left orientation engine, fire main engine, fire right orientation engine. \"\n",
        "\n",
        "着陸パッドの外に着陸することも可能です。燃料は無限であるため、エージェントは飛行を学習し、最初の試みで着陸することができます。何もしない」「左旋回エンジン起動」「メインエンジン起動」「右旋回エンジン起動」の4つの個別アクションが可能です。\n",
        "\n",
        ">Lunar Lander environment: [https://gym.openai.com/envs/LunarLander-v2/](https://gym.openai.com/envs/LunarLander-v2/)\n",
        "\n",
        "![Lunar Lander](https://cdn-images-1.medium.com/max/960/1*f4VZPKOI0PYNWiwt0la0Rg.gif)\n",
        "\n",
        ">Note: vectorized environments allow to easily multiprocess training. In this example, we are using only one process, hence the DummyVecEnv.\n",
        "\n",
        "注意：ベクトル化された環境では、簡単にマルチプロセスで学習することができます。この例では、1つのプロセスしか使っていないので、DummyVecEnv.\n",
        "\n",
        ">We chose the MlpPolicy because input of CartPole is a feature vector, not images.\n",
        "\n",
        "CartPoleの入力は画像ではなく、特徴ベクトルなので、MlpPolicyを選択しました。\n",
        "\n",
        ">The type of action to use (discrete/continuous) will be automatically deduced from the environment action space\n",
        "\n",
        "使用するアクションの種類（離散/連続）は、環境のアクション空間から自動的に推測されます。\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUWGZp3i9wyf"
      },
      "source": [
        "env = gym.make('LunarLander-v2')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create model"
      ],
      "metadata": {
        "id": "Uy6fxK1sgG6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = DQN('MlpPolicy', env, learning_rate=1e-3, prioritized_replay=True, verbose=1)"
      ],
      "metadata": {
        "id": "PTJX28wsgKFt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4efFdrQ7MBvl"
      },
      "source": [
        "We create a helper function to evaluate the agent:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define evaluate"
      ],
      "metadata": {
        "id": "TykJPG1NgSLt"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "63M8mSKR-6Zt"
      },
      "source": [
        "def evaluate(model, num_steps=1000):\n",
        "  \"\"\"\n",
        "  Evaluate a RL agent\n",
        "  :param model: (BaseRLModel object) the RL Agent\n",
        "  :param num_steps: (int) number of timesteps to evaluate it\n",
        "  :return: (float) Mean reward for the last 100 episodes\n",
        "  \"\"\"\n",
        "  episode_rewards = [0.0]\n",
        "  obs = env.reset()\n",
        "  for i in range(num_steps):\n",
        "      # _states are only useful when using LSTM policies\n",
        "      action, _states = model.predict(obs)\n",
        "\n",
        "      obs, reward, done, info = env.step(action)\n",
        "      \n",
        "      # Stats\n",
        "      episode_rewards[-1] += reward\n",
        "      if done:\n",
        "          obs = env.reset()\n",
        "          episode_rewards.append(0.0)\n",
        "  # Compute mean reward for the last 100 episodes\n",
        "  mean_100ep_reward = round(np.mean(episode_rewards[-100:]), 1)\n",
        "  print(\"Mean reward:\", mean_100ep_reward, \"Num episodes:\", len(episode_rewards))\n",
        "  \n",
        "  return mean_100ep_reward"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zjEVOIY8NVeK"
      },
      "source": [
        ">Let's evaluate the un-trained agent, this should be a random agent.\n",
        "\n",
        "訓練されていないエージェントを評価しましょう。これはランダムなエージェントであるべきです。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xDHLMA6NFk95",
        "outputId": "14b0a6c6-32b0-4553-9c6d-81b4f9e1fff4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Random Agent, before training\n",
        "mean_reward_before_train = evaluate(model, num_steps=10000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean reward: -629.5 Num episodes: 112\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ランダムなので相当結果悪いですねwww"
      ],
      "metadata": {
        "id": "WL8z9oosgiFa"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5UoXTZPNdFE"
      },
      "source": [
        "## Train the agent and save it (I)\n",
        "\n",
        "まずは10000回学習させてみます．"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4cfSXIB-pTF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02606d86-8933-4458-cb08-d1cd6a2c7815"
      },
      "source": [
        "# Train the agent\n",
        "model.learn(total_timesteps=10000, log_interval=10)\n",
        "# Save the agent\n",
        "model.save(\"dqn_lunar10000\")\n",
        "del model  # delete trained model to demonstrate loading"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------\n",
            "| % time spent exploring  | 9        |\n",
            "| episodes                | 10       |\n",
            "| mean 100 episode reward | -358     |\n",
            "| steps                   | 920      |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 20       |\n",
            "| mean 100 episode reward | -311     |\n",
            "| steps                   | 2379     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 30       |\n",
            "| mean 100 episode reward | -239     |\n",
            "| steps                   | 6428     |\n",
            "--------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T31dZJYNrJwF"
      },
      "source": [
        "## Load the trained agent"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10000回学習させたモデルをロードしてみます．"
      ],
      "metadata": {
        "id": "NpdYKSpphss6"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1ExgtyZrIA6"
      },
      "source": [
        "model = DQN.load(\"dqn_lunar10000\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ランダムよりはマシになってますね．"
      ],
      "metadata": {
        "id": "x8fHIawQhxzl"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygl_gVmV_QP7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2ef0e21-dde1-43ac-e49c-05f34170ba82"
      },
      "source": [
        "# Evaluate the trained agent\n",
        "mean_reward = evaluate(model, num_steps=10000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean reward: -74.1 Num episodes: 11\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the agent and save it (II)"
      ],
      "metadata": {
        "id": "x12-1Taqhewl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "モデルに学習環境をセットします．\n",
        "こいつの存在に辿り着くまでに結構時間がかかりました．"
      ],
      "metadata": {
        "id": "dxcKJMwQh34b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.set_env(env)"
      ],
      "metadata": {
        "id": "p3Hl6plLh3PM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ここからは先ほどと同じですね．追加で学習するので，ファイル名は20000にしておきます．"
      ],
      "metadata": {
        "id": "A1Hcimk_iAwf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aQDZI5VEGnUq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a54a76a8-4165-4903-bd8b-acb1aa14f12f"
      },
      "source": [
        "# Train the agent\n",
        "model.learn(total_timesteps=10000, log_interval=10)\n",
        "# Save the agent\n",
        "model.save(\"dqn_lunar20000\")\n",
        "del model  # delete trained model to demonstrate loading"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 10       |\n",
            "| mean 100 episode reward | -61.2    |\n",
            "| steps                   | 1821     |\n",
            "--------------------------------------\n",
            "--------------------------------------\n",
            "| % time spent exploring  | 2        |\n",
            "| episodes                | 20       |\n",
            "| mean 100 episode reward | -63.5    |\n",
            "| steps                   | 9977     |\n",
            "--------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "20000回学習させたモデルをロードしてみます．"
      ],
      "metadata": {
        "id": "vKdkiG-BiLwR"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAsDE_z0iLwR"
      },
      "source": [
        "model = DQN.load(\"dqn_lunar20000\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "いい感じでちゃんと学習が引き継がれているのではないでしょうか．"
      ],
      "metadata": {
        "id": "v3E1PYI8iLwT"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rbXfzzyQiLwT",
        "outputId": "4ec6d085-ba47-4906-cb6d-eccdc0e8c92d"
      },
      "source": [
        "# Evaluate the trained agent\n",
        "mean_reward = evaluate(model, num_steps=10000)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean reward: -22.3 Num episodes: 14\n"
          ]
        }
      ]
    }
  ]
}